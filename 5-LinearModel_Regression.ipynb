{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02bfee3-54db-49a1-a37c-5a944caead13",
   "metadata": {},
   "source": [
    "# Linear Models for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded1272-f0a5-4ea2-990e-ecc98d0cec82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4fdb5-8216-46e8-99a0-a9a6bbc46293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/Rv_daily_lec4.csv\", index_col=0)\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9c22d-187b-4cfc-ab4d-3f21f2031d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469de19f-2d94-407e-ae40-24899db5541d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e098c3-bb74-4507-a762-48661f0e7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b747d88-4cab-4b05-aa7d-fc68965bd042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var = \"RV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f2d93-5545-4e3a-81f3-63af206fcb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot the data.\n",
    "df.plot.scatter(x=\"Return_close\", y=var, title=\"SP500 ret vs {}\".format(var));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b634d4-801f-4e10-9ee2-2d7536eb6b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scatter plot the data.\n",
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"Return_close\", y=var, title=\"SP500 ret vs {}\".format(var), ax=ax)\n",
    "ax.set_xlim(-10, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff61b96-72a8-40a5-b860-539768ed1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9452c-3d51-42f4-8789-95d1fd7d60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the Pearson correlation coefficient.\n",
    "df[[\"Return_close\", 'RV']].corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d398fd6-9a87-4384-984f-c652ad0ec9ee",
   "metadata": {},
   "source": [
    "IMPORTANT: With a linear correlation coefficient of -0.15, we aim to suggest the presence of a negative linear relationship (though the possibility of a nonlinear relationship cannot be ruled out). However, it is important to emphasize that conclusions should not be drawn solely based on the initial interpretation of statistical values without further analysis or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cb2d4-810e-4546-8e1d-6df37bac9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RV'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b643993-5d31-4a7a-a0bc-a8eea891f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Return_close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b0317-3e13-43ef-9229-3ab794e9ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(r, P_value) = pearsonr(df[\"Return_close\"], df['RV'])\n",
    "print(\"correlation = %f\" % r)\n",
    "print(\"P_value = %f\" % P_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b44ac-a884-4db2-9a01-079ebfbceb22",
   "metadata": {},
   "source": [
    "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 200 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41446822-fe70-4665-ab34-b0e16d6996d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaf292-3cb8-4fc5-aee0-c47a2c91bdbc",
   "metadata": {},
   "source": [
    "# Univariate Linear Regression\n",
    "\n",
    "The first - and arguably the most straightforward - statistical model that we will face is the univariate linear regression model. We have one continuous predictor - also known as the independent variable - and one continuous the dependent variable. The task changes depending on the values that those variables assume. If the dependent variable is assumed to be unbounded, i.e., taking values across the whole domain of real numbers, we are solving a **regression** problem. \n",
    "\n",
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">What does it happen if the independent variable is categorical?</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "Linear regression does not imply any causality; it is up to the model user to impose causal assumptions, i.e., which variable takes the role of the criterion and which variable is assigned as a predictor. It is unnecessary to set any such assumptions to obtain a valid linear regression model. However, it is very customary to have some hypothesized direction of causality to discuss prediction meaningfully.\n",
    "\n",
    "Like any other statistical model, linear regression rests upon some assumptions. We will discuss the following more thoroughly and learn how to assess their validity during this session:\n",
    "\n",
    "* **Linearity**: The relationship between the independent and dependent variables is linear. This also means that the effects of the changes in the independent variable(s) on the dependent variable are constant.;\n",
    "* **Normal distribution of residuals (model errors)**: The errors (residuals of the model) follow a Normal distribution.;\n",
    "* **Constant variance: homoscedasticity**:The variance of the errors is constant across all levels of the independent variables. This means that the 'spread' of the residuals should remain constant and not form a funnel-like shape.;\n",
    "* **Independence of errors == no autocorrelation of residuals**;\n",
    "* **No significant outliers or influential cases**;\n",
    "\n",
    "\n",
    "Let's come up with our initial linear regression model:\n",
    "\n",
    "\n",
    "1. One variable, denoted `x`, is regarded as the predictor, explanatory, or independent variable.\n",
    "2. The other variable, denoted `y`, is regarded as the response, outcome, or dependent variable.\n",
    "\n",
    "In this case, `x` could be any of the explanatory variables like `TBill1Y`, `TBill3M`, `Oil`, `RV`, `Gold`, `SP_volume`, `weekday` and 'y' could be `Return_close`, if you're trying to predict returns based on these factors.\n",
    "\n",
    "The simple linear regression model provides a coefficient estimate that quantifies the direction and strength of the relationship between the predictor variable and the response. The estimated regression function (black line) has the equation:\n",
    "\n",
    "$$y_t = \\alpha + \\beta x_t + \\epsilon_t$$\n",
    "\n",
    "$\\alpha$ and $\\beta$ are two unknown parameters that represent the intercept and slope terms in the linear model.\n",
    "\n",
    "We will use the `statsmodels` package to conduct simple linear regression. We choose the realized volatility (RV) as the response variable variable and the S&P500 returns as the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d777f-fdd1-4212-bdf2-21a39f62d84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[\"Return_close\"]\n",
    "y = df[\"RV\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15af3c-5b80-4453-8184-03d1cdceccee",
   "metadata": {},
   "source": [
    "Before we perform the regression, we need to add a constant to our X variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1924a-9209-4139-8fa7-1ee9f2922355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7113406-78d3-4c37-8468-40043cca86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259d9db-c25a-4d9a-8c1c-99bebfedafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dd3b0-15fa-4581-b4cd-8b03c6f5e2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d012cd-a36a-4f06-9075-9198334ecb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978db7e-ab3d-4824-b132-f30b00b85290",
   "metadata": {},
   "source": [
    "$$\\hat{y} = 1.1432 - 0.0664 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818f48e-90ff-4711-928b-8314453cde8b",
   "metadata": {},
   "source": [
    "Look at the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7a930-f1a3-4b76-8903-a34b95a20fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.resid  # from stats object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d85db-f26e-4553-bc52-03012c8e7a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "residuals = results.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b04d63-0885-416b-bd9c-083b979884ac",
   "metadata": {},
   "source": [
    "**Test for Error Normality**\n",
    "\n",
    "One of the main assumption for the inferential part of the regression (OLS - ordinary least\n",
    "squares) is the assumption that the errors follow a normal distribution. A first important\n",
    "verification is to check the compatibility of the residuals (the errors observed on the sample)\n",
    "with this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa48bd9f-4ea3-492e-9bbc-961853179206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.displot(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d00e02-5d31-43be-a851-9ea52dd8f8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = residuals.mean()\n",
    "std = residuals.std()\n",
    "x = residuals.sort_values()\n",
    "plt.hist(residuals, bins=100, density=1)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, std), \"red\")\n",
    "plt.xlabel(\"residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23688b-5ce8-400d-9007-3963f7174b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Residuals sklearn\n",
    "print(\"skewness -> %f\" % residuals.skew())\n",
    "print(\"excess kurtosis -> %f\" % residuals.kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3311451-432d-436d-8364-19970ced4d3d",
   "metadata": {},
   "source": [
    "**Jarque-Bera normality test  (uses only skewness and kurtosis)**\n",
    "\n",
    "If the data comes from a normal distribution, the JB statistic **asymptotically** has a **Chi-Squared** distribution with 2 degrees of freedom\n",
    "$$JB = \\frac{n-k}{6}(\\xi^2+\\frac 1 4(\\chi -3)^2) $$\n",
    "where $n$ is the number of observations and $k$ is the number of regressors when examining residuals to an equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2440c-7a8f-435f-8f8c-e3cc8c4c5342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JB, JBpv, skw, kurt = sm.stats.stattools.jarque_bera(residuals)\n",
    "print(JB, JBpv, skw, kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84206aa9-66ff-4186-a9ef-4dc6943f079b",
   "metadata": {},
   "source": [
    "Here we reject the null hypotesis that the errors follow a normal distribution.\n",
    "\n",
    "**Q-Q plot**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2ec5d-5b95-43df-9293-e3ac11994e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.qqplot(residuals, line=\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824166d-3ed4-41b7-9c1b-82d6c3ef3f93",
   "metadata": {},
   "source": [
    "**K-S test**\n",
    "\n",
    "The Kolmogorov–Smirnov statistic for a given cumulative distribution function $F(x)$ is:\n",
    "$$D_n = \\sup_x |F_n(x)- F(x)|$$\n",
    "Asymptotically $\\sqrt {n}D_{n}$ converges to the Kolmogorov distribution, which does not depend on F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310b012-0fc6-4710-b63d-efd42a135219",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689846c4-bb57-4e3e-92cf-a9f535ca15ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(KS, p_V) = stats.kstest(residuals, \"norm\")\n",
    "print(\"KS -> %f\" % (KS))\n",
    "print(\"p_V -> %f\" % (p_V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1ea47-5aa0-4fd8-b79e-5a9f8533b62b",
   "metadata": {},
   "source": [
    "**Homoskedasticity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3fa6f-a379-4344-9520-d8b72539c39d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(residuals,X.iloc[:, 1], )\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8c834-f35b-49ff-ac86-77be630a5647",
   "metadata": {},
   "source": [
    "**Durbin-Watson test**\n",
    "\n",
    "The null hypothesis of the test is that there is no serial correlation. The Durbin-Watson test statistics is defined as:\n",
    "\n",
    "$$DB = \\frac{\\sum_t (e_t-e_{t-1})^2}{\\sum_t e_t^2}$$\n",
    "The test statistic is approximately equal to $2*(1-r)$ where $r$ is the sample autocorrelation of the residuals. Thus, for $r = 0$, indicating no serial correlation, the test statistic equals $2$. This statistic will always be between 0 and 4. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4, the more evidence for negative serial correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b103a-a54d-43e0-885d-199b24eddf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X[\"res\"] = results.resid\n",
    "E = X.sort_values(by=\"Return_close\").res\n",
    "print(\"DB_e -> %f\" % durbin_watson(E))\n",
    "print(\"DB_e^2 -> %f\" % durbin_watson(E**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c4034-4277-4e38-9adc-0a5ba0027bc3",
   "metadata": {},
   "source": [
    "If the assumptions of linearity, independence, homoscedasticity, and normality are violated, you might need to consider data transformations, adding interaction terms or applying a more suitable modeling technique.\n",
    "\n",
    "Please note that real-world data often violate the assumptions to some degree but still result in useful models. It's the degree of violation that determines whether we can overlook the violation or need to address it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44278ae-2570-48e8-a0cb-6be3a8f0d92e",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "Multivariate linear regression is an extension of univariate linear regression used to predict an outcome variable (Y) based on multivariate distinct predictor variables (X). With three or more variables involved, the data is modelled as a hyperplane in multidimensional space.\n",
    "\n",
    "The multivariate linear regression equation is as follows:\n",
    "\n",
    "$$y_{t} = \\alpha + \\beta_1 x_{t1} + \\beta_2 x_{t2} + ... + \\beta_n x_{tn} + \\epsilon_t $$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $y_t$ is the dependent variable.\n",
    "- $\\alpha$, $\\beta_1$, ..., $\\beta_n$ are the regression coefficients. They represent the change in the dependent variable for every one unit change in an independent variable, assuming all other variables are held constant.\n",
    "- $x_{t1}$, $x_{t2}$, ..., $x_{tn}$ are independent variables.\n",
    "- $\\epsilon$ is the error term (residuals).\n",
    "\n",
    "Just as with univariate linear regression, the assumptions for multivariate linear regression are linearity, independence, homoscedasticity, and normality of residuals.\n",
    "\n",
    "In a more compact form one can write\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}$$\n",
    "\n",
    "\n",
    "Remember, when using multivariate linear regression, multicollinearity can be a problem. Multicollinearity is when predictor variables are correlated with each other. This can be checked by examining the correlation matrix of the variables. If multicollinearity is found, you might need to remove one of the correlated variables or perform dimensionality reduction.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">Can I have both continuous and categorical predictors in a multivariate linear regression?</span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c4268-294a-40ac-a59d-84453183496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c840bc-dc63-49f9-9d68-cf90acb06ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TBill3M_ret'] = df['TBill3M'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd7319-fb27-435a-89b0-a1fe75335176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TBill3M_ret'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a704e7-2426-4dd0-ae69-df8b699b20bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=0.3,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178e941-8630-42f7-a7c4-979af049e5dc",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">Why does Tbill3M_ret has a very low correlation with the rest of the dataset?</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353a6ca-4eef-4f2c-87b9-a15c74f26302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_to_transform = [\"TBill3M\", \"TBill1Y\", \"Oil\", \"Gold\", \"SP_volume\"]\n",
    "for c in col_to_transform:\n",
    "    df[\"{}_ret\".format(c)] = df[c].pct_change(1) * 100\n",
    "\n",
    "df = df.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350be2a-06ba-49ab-a04b-0a09390594b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    cmap=cmap,\n",
    "    vmax=0.3,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985127e-ba06-4a23-aac7-06190a65f116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define predictor variables and the response variable\n",
    "X = df[[\"RV\", \"TBill1Y_ret\"]]\n",
    "y = df[\"Return_close\"]\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Build the model\n",
    "model = sm.OLS(y, X)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa81970-c72d-4c3a-8331-d8fcee6d35d1",
   "metadata": {},
   "source": [
    "## Autoregressive Components and Predictive Modeling\n",
    "\n",
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">What about including an autoregressive component to the model?</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf64673-c9d2-4ce7-a19f-71f555005f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift down, create lags\n",
    "df['Return_close_t-1'] = df['Return_close'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cb9ca-36ce-4c35-b4be-0fdb750ca508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead4ca7-1f73-4c4e-89f0-c14d251223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift up, create future targets\n",
    "df['RV_t+1'] = df['RV'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e654aea-d5e8-4c9d-86eb-68bc9922630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823646f3-1d80-4313-952a-1b696ac148da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ea4fb-67f1-4873-8644-0dfb038ca128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['RV','RV_t+1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57f369-409b-4dcf-9a96-084eab95a99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_lags(df, columns, n_lags=1):\n",
    "    \"\"\"\n",
    "    Add lags to specific columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Original DataFrame.\n",
    "    - columns (list): List of column names for which to create lags.\n",
    "    - n_lags (int): Number of lags to create for each column.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated DataFrame with lag columns.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for column in columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df_copy.loc[:,f\"{column}_lag{lag}\"] = df_copy.loc[:,column].shift(lag)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae20b7-5070-430f-a14c-9cba7ee85e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_lags(df, [\"Return_close\"], n_lags=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a442e-076b-4bd9-9a2c-7104fdf68cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426fd41-fe9f-4a8a-9c5f-a20410d36d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ad49d-0049-4ae9-98d6-df884b0d47d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0330d-bbf2-4c32-aff1-561e905e4573",
   "metadata": {},
   "source": [
    "Using `RV` and `TBill1Y_ret` as regressors will not make the estimated model a predictive one. \n",
    "\n",
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">Why will not be the model predictive?</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bffa0-ef26-4932-bb50-6d466cf5fe27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_cols = [\n",
    "    # \"RV\",\n",
    "    # \"TBill1Y_ret\",\n",
    "    \"Return_close_lag1\",\n",
    "    \"Return_close_lag2\",\n",
    "    \"Return_close_lag3\",\n",
    "]\n",
    "X = df[X_cols]\n",
    "y = df[\"Return_close\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f3d9f-392c-4c67-91ee-8ebc695ddd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_cols = [\"Return_close_lag1\", \"Return_close_lag2\"]\n",
    "X = df[X_cols]\n",
    "y = df[\"Return_close\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d680e-520b-439b-a684-f9eabe76fdfa",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28323f2-0645-439a-9ccc-44eb90e43dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb404894-2780-4f51-b229-4424512cc688",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = [\"Return_close_lag1\", \"Return_close_lag2\"]\n",
    "X = df[X_cols]\n",
    "y = df[\"Return_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7ecec-ef3f-413b-a4bf-ebb3e60b6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index for the 80/20 train-test split\n",
    "train_size = int(0.8 * len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19633e-51b6-4fc8-85ad-5551b28ed225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c346835-1939-4bd2-aeac-1fa7cdee4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830310b-4640-4aa0-a640-5de43e93f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = results.predict(X_train)\n",
    "y_test_pred = results.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833610b-a41e-4d82-9cc6-d6552834b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on training and testing data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train MSE: {mse_train}\")\n",
    "print(f\"Test MSE: {mse_test}\")\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51893471-aead-46e9-b93f-233639b2a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(f\"Train RMSE: {rmse_train}\")\n",
    "print(f\"Test RMSE: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3035e30-878a-433e-a2ff-212cc4afbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf964d8-c4cd-42f4-b2e3-ae563db2b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual')\n",
    "plt.plot(y_test.index, y_test_pred, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Return_close')\n",
    "plt.title('Actual vs. Predicted Returns on Test Set')\n",
    "plt.legend()\n",
    "plt.xticks(y_test.index[::200]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64bf5f-d7d5-4b71-9d1e-591667b6e957",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center; font-size:24px\">\n",
    "    <span style=\"color:red\">How can this model be improved?</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ae578-e017-4f18-9cd4-de7ccea061dc",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "Polynomial Regression is a form of linear regression in which the relationship between $X$ and $Y$ is modeled as an $n$-th degree polynomial. Polynomial regression fits a nonlinear relationship between the independent variable and the depedent variables.\n",
    "\n",
    "Even though it models a nonlinear relationship, Polynomial Regression is still considered a linear model because the regression function is linear in terms of the coefficients:\n",
    "\n",
    "$$ y = \\alpha + \\beta_1 x_1 + \\beta_2 x_1^2 + \\dots + \\beta_n x_1^n $$\n",
    "\n",
    "Here:\n",
    "- $\\alpha, \\beta_1, \\dots, \\beta_n$ are the coefficients.\n",
    "- $x_1, \\ldots, x_1^n$ are the independent variables.\n",
    "\n",
    "As the degree of the polynomial increases, the model can fit a wider range of curvatures, making it more flexible. However, high-degree polynomials might lead to overfitting, where the model performs poorly on new, unseen data.\n",
    "\n",
    "Polynomial Regression captures these relationships to an nth-degree polynomial. This allows for a more complex interplay between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edd5bd-0a0d-4dbf-8a85-436d0c737642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2)  \n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "\n",
    "# Fit a Polynomial Linear Regression model \n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y)\n",
    "\n",
    "# Get the model parameters\n",
    "intercept = poly_reg.intercept_\n",
    "coefficients = poly_reg.coef_\n",
    "\n",
    "print(\"Intercept: \\n\", intercept)\n",
    "print(\"Coefficients: \\n\", coefficients)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = poly_reg.predict(X_poly)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error: \\n\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660038a-796c-4ddb-b205-38df37b79761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coefficients.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f339a-27ed-48de-83ae-fe11dcb863e1",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e355285-6a98-4b6b-a049-4482cea15c49",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge Regression is a technique used when the data suffers from multicollinearity (independent variables are highly correlated). By adding a degree of bias to the regression estimates, Ridge Regression reduces the standard errors. \n",
    "\n",
    "This technique works by adding a \"squared magnitude\" of coefficient as penalty term to the loss function. \n",
    "\n",
    "**Ridge Regression** aims to minimize the following objective function:\n",
    "\n",
    "$$L(\\beta) = ||Y - X\\beta||^2 + \\lambda||\\beta||^{2}_{2} $$\n",
    "\n",
    "where:\n",
    "- $Y$ is the response variable.\n",
    "- $X$ is the design matrix.\n",
    "- $\\beta$  is the vector of coefficients.\n",
    "- $\\lambda$ is the Ridge regularization parameter.\n",
    "\n",
    "The term $\\lambda||\\beta||^2$ is the L2 penalty term that \"penalizes\" the size of the coefficients. While $\\lambda$ can take any value between 0 and $\\infty$, note that:\n",
    "- When $\\lambda = 0$, Ridge Regression will produce the same coefficients as a simple linear regression.\n",
    "- When $\\lambda = \\infty$, all coefficients will be zero because of infinite penalty.\n",
    "- When $0 < \\lambda < \\infty$, the magnitude of $\\lambda$ will decide the value of coefficients.\n",
    "\n",
    "**Differences in Optimization Compared to OLS**:\n",
    "\n",
    "Ordinary Least Squares (OLS) aims to minimize just the residual sum of squares:\n",
    "\n",
    "$$L_{OLS}(\\beta) = ||Y - X\\beta||^2$$\n",
    "\n",
    "As you can see, OLS does not have the regularization term that Ridge regression does. The L2 penalty in Ridge Regression shrinks the coefficients, especially when the regularization parameter $\\lambda$ is large, which can help prevent overfitting especially in scenarios where multicollinearity is present. One of the significant advantages of ridge regression is coefficient shrinkage and reducing model complexity.\n",
    "\n",
    "\n",
    "Here, $\\lambda$ is a tuning parameter (also known as regularization parameter) that decides how much we want to penalize the flexibility of our model.\n",
    "\n",
    "\n",
    "\n",
    "Generally, we use `sklearn` for Ridge estimation. The `statsmodels` library does not have a specific Ridge Regression function similar to `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d46163-be04-4509-958e-c97d9669a6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_cols = [\n",
    "    \"RV\",\n",
    "    \"TBill3M_ret\",\n",
    "    \"TBill1Y_ret\",\n",
    "    \"Oil_ret\",\n",
    "    \"Gold_ret\",\n",
    "    \"SP_volume_ret\",\n",
    "    \"Return_close_lag1\",\n",
    "    \"Return_close_lag2\",\n",
    "    \"Return_close_lag3\",\n",
    "]\n",
    "X = df[X_cols]\n",
    "y = df[\"Return_close\"]\n",
    "\n",
    "# Fit a Ridge regression model\n",
    "ridge_reg = Ridge(alpha=0.5)  \n",
    "ridge_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078df5c3-1ae6-44d2-9ab4-05b1ed40bc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the model parameters\n",
    "intercept = ridge_reg.intercept_\n",
    "coefficients = ridge_reg.coef_\n",
    "\n",
    "print(\"Intercept: \\n\", intercept)\n",
    "print(\"Coefficients: \\n\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58bb3a3-14c8-48e2-b075-993c99e9df92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = ridge_reg.predict(X)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error: \\n\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4a838-1e1c-4e71-a4fc-12c2e304194b",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "**Lasso (Least Absolute Shrinkage and Selection Operator) Regression** is another regularization technique. It's useful when dealing with feature selection in a model where we have a large number of features.\n",
    "\n",
    "Like Ridge Regression, Lasso also adds a penalty for non-zero coefficients, but unlike Ridge regression which penalizes sum of squared coefficients (the L2 penalty), lasso penalizes the sum of their absolute values (the L1 penalty). As a result, for high values of \\( \\lambda \\), many coefficients are exactly zeroed under Lasso, which is never the case in Ridge.\n",
    "\n",
    "**Objective Function for Lasso Regression**:\n",
    "\n",
    "$$L(\\beta) = ||Y - X\\beta||^2 +  \\lambda  || \\beta ||_{1} $$\n",
    "\n",
    "Where:\n",
    "- $ \\lambda $ is the Lasso regularization parameter.\n",
    "- $ \\beta_j $ are the model coefficients.\n",
    "\n",
    "While $ \\lambda $ can take any value between 0 and $\\infty$, note that:\n",
    "- When $ \\lambda = 0 $, Lasso produces the same coefficients as a simple linear regression.\n",
    "- When $ \\lambda = \\infty $, all coefficients are zero because of infinite penalty.\n",
    "- When $ 0 < \\lambda < \\infty $, the magnitude of $ \\lambda $ will decide how the model balances fit with complexity.\n",
    "\n",
    "The key difference from Ridge Regression is the L1 penalty can lead to zero coefficients i.e. some of the features are completely eliminated, hence providing a feature selection. This is a useful property for machine learning applications where feature selection is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b02ae2-8d90-4250-ad57-76de8fbf56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039859e-38d2-4ba5-a4ad-a265859f4074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "# Get the model parameters\n",
    "intercept = lasso_reg.intercept_\n",
    "coefficients = lasso_reg.coef_\n",
    "\n",
    "print(\"Intercept: \\n\", intercept)\n",
    "print(\"Coefficients: \\n\", coefficients)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lasso_reg.predict(X)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error: \\n\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c6bde-1ce4-44ba-a0bb-ba65eae42d1d",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "\n",
    "\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso. It incorporates penalties from both Lasso and Ridge to get the best of both worlds. Elastic Net aims at minimizing the following loss function:\n",
    "\n",
    "$$L(\\beta) = ||Y - X\\beta||^2 +  \\lambda_1 || \\beta ||_{1} + \\lambda_2  ||\\beta||^{2}_{2}  $$ \n",
    "\n",
    "Where:\n",
    "- $\\lambda_1$ is the coefficient of L1 penalty, similar to the one used in Lasso.\n",
    "- $\\lambda_2$ is the coefficient of L2 penalty, similar to the one used in Ridge.\n",
    "\n",
    "In other words, Elastic Net is a hybrid of Ridge Regression and Lasso. It works by penalizing the model using both the L2-norm (Ridge) and the L1-norm (Lasso). \n",
    "\n",
    "The key takeaway is that Elastic Net is useful when there are multivariate features which are correlated. Lasso might randomly pick one of these, but elastic-net will take both of them into account. However, it does have a computational cost as it adds an extra hyperparameter to tune.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd07b4-7367-44a0-aece-91edc112297a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit an Elastic Net model\n",
    "elastic_reg = ElasticNet(\n",
    "    alpha=0.1, l1_ratio=0.5\n",
    ")\n",
    "elastic_reg.fit(X, y)\n",
    "\n",
    "# Get the model parameters\n",
    "intercept = elastic_reg.intercept_\n",
    "coefficients = elastic_reg.coef_\n",
    "\n",
    "print(\"Intercept: \\n\", intercept)\n",
    "print(\"Coefficients: \\n\", coefficients)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = elastic_reg.predict(X)\n",
    "\n",
    "# Calculate the mean squared error of the predictions\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error: \\n\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119b1ca-3c21-45f0-bc70-129bf3159886",
   "metadata": {},
   "source": [
    "# Appendix: Background Theory\n",
    "\n",
    "Models from econometrics, which is the intersection of Economics and Statistics.An econometric model is an association between $y_{i}$ and $x_{i}$ E.g.:\n",
    "- personal income $y_{i}$ and personal QI $x_{i}$\n",
    "- stock return $y_{i}$ and market return $x_{i}$\n",
    "- current return $y_{t}$ and past returns $y_{t-h}$\n",
    "\n",
    "The econometric model provides an \"approximate,\" i.e., a probabilistic description of the association. The relation will be stochastic and not deterministic. Econometrics provides estimation methods for the parametric model.\n",
    "\n",
    "**Ordinary least squares (OLS): a first linear model**\n",
    "\n",
    "Linear model\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{i} &=f\\left(x_{i 1}, x_{i 2}, \\ldots, x_{i k-1}\\right)+\\varepsilon_{i} \\\\\n",
    "&=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\cdots+\\beta_{K} x_{i k-1}+\\varepsilon_{i} \\quad i=1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "where\n",
    "- $y_{i}:$ dependent or explained variable (observed)\n",
    "- $x_{i}$ : regressors or covariates or explanatory variables (observed)\n",
    "- $\\varepsilon_{i}:$ error term or random disturbance (unobserved)\n",
    "- $\\beta_{i}:$ unknown parameters or regression coefficient (unobserved)\n",
    "\n",
    "$$\n",
    "y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\cdots+\\beta_{K} x_{i k-1}+\\varepsilon_{i}\n",
    "$$\n",
    "can be written in vector notation\n",
    "$$\n",
    "y_{i}=\\underbrace{x_{i}^{\\prime}}_{1 \\times k} \\underbrace{\\beta}_{k \\times 1}+\\varepsilon_{i}\n",
    "$$\n",
    "and in the even more compact matrix notation\n",
    "$$\n",
    "\\underbrace{Y}_{n \\times 1}=\\underbrace{X}_{n \\times k} \\underbrace{\\beta}_{k \\times 1}+\\varepsilon\n",
    "$$\n",
    "with\n",
    "\n",
    "**OLS assumptions**\n",
    "\n",
    "Standard OLS Assumptions:\n",
    "- H.1 Strict exogeneity of regressors: $\\mathbb{E}[\\varepsilon \\mid X]=0$\n",
    "Note: $\\varepsilon_{i}$ does not depend on any $x_{j}$, neither past nor future $x$ s\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathbb{E}[\\varepsilon \\mid X]=0 \\Rightarrow \\mathbb{E}[\\varepsilon]=0 \\text { (by Law of Total Exp } \\mathbb{E}[\\mathbb{E}[\\varepsilon \\mid x]]=\\mathbb{E}[\\varepsilon]) \\\\\n",
    "&\\mathbb{E}[\\varepsilon \\mid X]=0 \\Rightarrow \\mathbb{E}(X \\varepsilon)=\\underbrace{\\mathbb{E}[\\mathbb{E}(X \\varepsilon \\mid X)]}_{\\text {Law of Total Exp }}=\\underbrace{\\mathbb{E}[X \\mathbb{E}(\\varepsilon \\mid X)]}_{\\text {is measurable }}=\\underbrace{0}_{\\mathbb{E}(\\varepsilon \\mid X)=0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\mathbb{E}[\\varepsilon \\mid X]=0 \\Rightarrow \\mathbb{E}[y \\mid X]=X \\beta \\quad$ i.e. $X \\beta$ is the conditional mean of $y \\mid X$.\n",
    "- H.2 Identification condition: $X$ is $n \\times k$ with rank $k$ with probability 1\n",
    "- H.3 Spherical errors $\\operatorname{Var}[\\varepsilon \\mid X]=\\sigma^{2} / n$\n",
    "$\\Rightarrow$ homoscedastic: $\\operatorname{Var}\\left[\\varepsilon_{i} \\mid X\\right]=\\sigma^{2}, \\quad \\forall i=1, \\ldots, n$ and\n",
    "uncorrelated errors: $\\operatorname{Cov}\\left[\\varepsilon_{i} \\varepsilon_{j} \\mid X\\right]=0 \\quad \\forall i \\neq j$\n",
    "\n",
    "\n",
    "**OLS estimation**\n",
    "\n",
    "Goal: statistical inference on $\\beta$, e.g. estimate $\\beta$\n",
    "Least Square finds $\\beta$ that minimizes the sum of squared residuals in $Y=X \\beta+\\varepsilon$ :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S S &=\\sum_{i=1}^{n} \\varepsilon_{i}^{2}=\\varepsilon^{\\prime} \\varepsilon \\\\\n",
    "=&(Y-X \\beta)^{\\prime}(Y-X \\beta) \\\\\n",
    "=& Y^{\\prime} Y-2 X^{\\prime} Y \\beta+\\beta^{\\prime} X^{\\prime} X \\beta \\\\\n",
    "\\text { F.O.C. } \\quad &: \\quad-2 X^{\\prime} Y+2 X^{\\prime} X \\beta=0 \\\\\n",
    "& \\Rightarrow \\quad X^{\\prime}(Y-X \\beta)=0 \\\\\n",
    "& \\Rightarrow \\quad X^{\\prime} X \\beta=X^{\\prime} Y\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\Rightarrow$ OLS estimator:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta} &=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y \\\\\n",
    "&=\\left(\\sum_{i=1}^{n} x_{i} x_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} x_{i} y_{i}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Unbiasedness: $\\mathbb{E}[\\hat{\\beta} \\mid X]=\\beta$\n",
    "$$\n",
    "\\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime}(X \\beta+\\varepsilon)=\\beta+\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} \\varepsilon\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\mathbb{E}[\\hat{\\beta} \\mid X]=\\beta+\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} \\underbrace{\\mathbb{E}[\\varepsilon \\mid X]}_{=0(H .1)}=\\beta\n",
    "$$\n",
    "- Variance: $\\operatorname{Var}(\\hat{\\beta} \\mid X)=\\sigma^{2}\\left(X^{\\prime} X\\right)^{-1}$\n",
    "$$\n",
    "\\operatorname{Var}[\\hat{\\beta} \\mid X]=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} \\underbrace{\\operatorname{Var}[\\varepsilon \\mid X]}_{\\sigma^{2} I_{n}(H .3)} X\\left(X^{\\prime} X\\right)^{-1}=\\sigma^{2}\\left(X^{\\prime} X\\right)^{-1}\n",
    "$$\n",
    "- Efficiency (Gauss-Markov Theorem): $\\hat{\\beta}$ is $B L U E$, i.e. $\\operatorname{Var}(\\hat{\\beta} \\mid X) \\leq \\operatorname{Var}(\\tilde{\\beta} \\mid X), \\forall \\tilde{\\beta}$ linear unbiased estimator (prove it)\n",
    "\n",
    "**Goodness of fit**\n",
    "\n",
    "\n",
    "being $\\hat{Y} \\perp e$ then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}(Y) &=\\operatorname{Var}(\\hat{Y})+\\operatorname{Var}(e) \\\\\n",
    "\\frac{T S S}{n} &=\\frac{E S S}{n}+\\frac{R S S}{n} \\\\\n",
    "\\text { Total Var } &=\\text { Explained Var + Residual Var }\n",
    "\\end{aligned}\n",
    "$$\n",
    "A common measure of goodness of fit is the coefficient of determination $R^{2}$ :\n",
    "$$\n",
    "R^{2}=\\frac{\\text { Explained Var }}{\\text { Total Var }}=1-\\frac{\\text { Residual Var }}{\\text { Total Var }}=1-\\frac{R S S}{T S S}\n",
    "$$\n",
    "since $R^{2}$ always increases when a regressor is added (even if uncorrelated)\n",
    "$$\n",
    "\\text { Adjusted } R^{2}=1-\\frac{\\text { Residual Var } /(n-k)}{\\text { Total Var } /(n-1)}\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fintech_lectures",
   "language": "python",
   "name": "fintech_lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
