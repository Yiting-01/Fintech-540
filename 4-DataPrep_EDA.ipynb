{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giJdRTpW83Dq"
   },
   "source": [
    "# Data Preparation and Exploration\n",
    "\n",
    "Data **preparation** and **exploration** are the linchpins in any data analysis project, laying the critical groundwork for applying any quantitative modeling. Data preparation ensures the quality and accuracy of data, minimizing potential inaccuracies in subsequent analyses. This process includes tasks such as cleaning, handling missing values, and standardization, all designed to increase the reliability of results.\n",
    "\n",
    "On the other hand, data exploration facilitates a deeper understanding of the data's underlying structure and characteristics, informing the choice of appropriate models and guiding hypothesis formation. This step is key to identifying patterns, relationships, and anomalies.\n",
    "\n",
    "Fitting any models to you data risks yielding misleading or false insights without a solid foundation of well-prepared and thoroughly understood data. Despite being less glamorous than modeling itself, the importance of these preliminary steps cannot be overstated.\n",
    "\n",
    "The snippets of code provided serve multiple purposes for our data analysis pipeline:\n",
    "- Firstly, they allow for loading data obtained from various sources. \n",
    "- Following this, we can specify the time frame for our analysis. \n",
    "- The data is then aggregated for consistency and ease of analysis. \n",
    "- After aggregation, the code assists in identifying and removing missing values for a clean dataset. \n",
    "\n",
    "An important manual intervention worth noting is the replacement of \".\" with NaN, specifically for data procured from the [FRED database](https://fred.stlouisfed.org/), to ensure the correct identification and handling of missing data points.\n",
    "\n",
    "**REMARK**: You can find the files we will use here on the Canvas website for our class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtNVxH7I83Dr"
   },
   "source": [
    "## Import Modules and Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDDMDT-G83Ds",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Access individual DataFrame as: data['^GSPC'], data['GOLDPMGBD228NLBM'], etc.\n",
    "files = [\"^GSPC\", \"GOLDPMGBD228NLBM\", \"DCOILWTICO\", \"DGS3MO\", \"DGS1\"]\n",
    "data_dir = \"data/to_merge_lect4/\"\n",
    "data = {f: pd.read_csv(data_dir + f + \".csv\", index_col=0) for f in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    print(data[d].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g7UwRqt83Ds",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp = data[\"^GSPC\"]\n",
    "gld = data[\"GOLDPMGBD228NLBM\"]\n",
    "wti = data[\"DCOILWTICO\"]\n",
    "bill3m = data[\"DGS3MO\"]\n",
    "bill1y = data[\"DGS1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question:**  \n",
    "*Do you already have a sense of what those variables represents?*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to expand!</summary>\n",
    "  \n",
    "1. `sp = data[\"^GSPC\"]`\n",
    "   - S&P 500 index, which is a stock market index that measures the stock performance of 500 large companies listed on stock exchanges in the United States. The symbol `^GSPC` is commonly used to denote this index.\n",
    "   \n",
    "2. `gld = data[\"GOLDPMGBD228NLBM\"]`\n",
    "   - Gold price. The code `GOLDPMGBD228NLBM` is used to denote the price of gold per troy ounce in the London market, usually updated on business days by the Federal Reserve Bank of St. Louis.\n",
    "\n",
    "3. `wti = data[\"DCOILWTICO\"]`\n",
    "   - West Texas Intermediate (WTI) crude oil prices. `DCOILWTICO` is a commonly used code to represent the daily prices of WTI crude oil.\n",
    "\n",
    "4. `bill3m = data[\"DGS3MO\"]`\n",
    "   - 3-month Treasury bill rate. The code `DGS3MO` is used to denote the daily interest rate on a 3-month Treasury bill, which is a short-term debt obligation issued by the U.S. government.\n",
    "\n",
    "5. `bill1y = data[\"DGS1\"]`\n",
    "   - 1-year Treasury bill rate. The code `DGS1` is used to denote the daily interest rate on a 1-year Treasury bill, another type of short-term debt obligation issued by the U.S. government.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lnul5jQw83Ds",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_close_vol = sp[[\"Adj Close\", \"Volume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mh7UMAlf83Dt",
    "outputId": "1ef3a086-1725-413e-cef0-1d73fbd9445b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sp_close_vol.head())\n",
    "print(\"----------------------------------\")\n",
    "print(gld.head())\n",
    "print(\"----------------------------------\")\n",
    "print(wti.head())\n",
    "print(\"----------------------------------\")\n",
    "print(bill3m.head())\n",
    "print(\"----------------------------------\")\n",
    "print(bill1y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sp_close_vol.tail())\n",
    "print(\"----------------------------------\")\n",
    "print(gld.tail())\n",
    "print(\"----------------------------------\")\n",
    "print(wti.tail())\n",
    "print(\"----------------------------------\")\n",
    "print(bill3m.tail())\n",
    "print(\"----------------------------------\")\n",
    "print(bill1y.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf59LzKu83Du",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df = pd.read_excel(\"{}/intradayRV.xlsx\".format(data_dir), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r071YdqV83Du",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df.index = pd.to_datetime(rv_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSnCv9lf83Du",
    "outputId": "cf4a32b1-0cfe-4aa3-e426-9b15bdeefed8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3emRJI1r83Du",
    "outputId": "d3c3f96e-8444-4c1c-daba-6c451572d991",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `add_overnight()`, corrects for overnight bias in financial data, typically realized volatility estimates. It does this by scaling the Close-to-Close Realized Volatility (CRV) by a bias correction factor.\n",
    "\n",
    "Here's a step-by-step breakdown of what the function does:\n",
    "\n",
    "- `meanAbsRet = np.sqrt(((returns**2).mean())*252)`: This line calculates the root mean square of the daily returns, scaled up to annual returns by multiplying with 252 (the typical number of trading days in a year). This gives an annualized measure of the average absolute return.\n",
    "\n",
    "- `meanCRV = np.sqrt((CRV**2).mean())`: This line calculates the root mean square of the Close-to-Close Realized Volatility.\n",
    "\n",
    "- `biasCorrFactor = meanAbsRet/meanCRV`: This calculates the bias correction factor by taking the ratio of meanAbsRet to meanCRV. If meanAbsRet is greater than meanCRV, it indicates that the CRV might be underestimating the actual volatility (likely due to ignoring overnight return movements), and the biasCorrFactor will be greater than 1. Conversely, if meanAbsRet is less than meanCRV, the biasCorrFactor will be less than 1, indicating a potential overestimation by the CRV.\n",
    "\n",
    "- `CRV = CRV*biasCorrFactor`: The CRV is then scaled by this biasCorrFactor to adjust for the detected bias.\n",
    "\n",
    "The function returns the corrected CRV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-pDrA0Z83Du",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_overnight(CRV, returns):\n",
    "    meanAbsRet = np.sqrt(((returns**2).mean()) * 252)\n",
    "\n",
    "    meanCRV = np.sqrt((CRV**2).mean())\n",
    "\n",
    "    biasCorrFactor = meanAbsRet / meanCRV\n",
    "\n",
    "    CRV = CRV * biasCorrFactor\n",
    "\n",
    "    return CRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNAWEbx283Dv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df[\"CRV_over\"] = add_overnight(rv_df[\"CRV\"], rv_df[\"Return_close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEMj4A-f83Dv",
    "outputId": "339d796a-e1cd-4625-e0d3-e8b61236ecd6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXWzIGs483Dv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df[\"CRV_over\"] = rv_df[\"CRV_over\"] / np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8oq6gGR83Dv",
    "outputId": "9b2ca504-f14f-443a-b702-cec4cccc892e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PPEiNPn83Dv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "RealVol = rv_df[[\"CRV_over\", \"Return_close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqxP-F8y83Dv",
    "outputId": "c561cc98-ae71-49d9-8e8b-896aa1acd65e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "RealVol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLbkbs_q83Dv"
   },
   "source": [
    "## Period selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL83nb9483Dv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = \"1986-01-02\"\n",
    "end_date = \"2009-02-04\"\n",
    "\n",
    "sp_close_vol = sp_close_vol.loc[start_date:end_date].copy()\n",
    "sp_close_vol.index = pd.to_datetime(sp_close_vol.index)\n",
    "\n",
    "gld_price = gld.loc[start_date:end_date].copy()\n",
    "gld_price.index = pd.to_datetime(gld_price.index)\n",
    "\n",
    "wti_price = wti.loc[start_date:end_date].copy()\n",
    "wti_price.index = pd.to_datetime(wti_price.index)\n",
    "\n",
    "bill3m_rate = bill3m.loc[start_date:end_date].copy()\n",
    "bill3m_rate.index = pd.to_datetime(bill3m_rate.index)\n",
    "\n",
    "bill1y_rate = bill1y.loc[start_date:end_date].copy()\n",
    "bill1y_rate.index = pd.to_datetime(bill1y_rate.index)\n",
    "\n",
    "RealVol = RealVol.loc[start_date:end_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5Y-tZ7I83Dw",
    "outputId": "1f2d7ea1-9922-4bcb-f7fc-33f1ba5e260a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sp_close_vol.head(1))\n",
    "print(\"----------------------------------\")\n",
    "print(gld_price.head(1))\n",
    "print(\"----------------------------------\")\n",
    "print(wti_price.head(1))\n",
    "print(\"----------------------------------\")\n",
    "print(bill3m_rate.head(1))\n",
    "print(\"----------------------------------\")\n",
    "print(bill1y_rate.head(1))\n",
    "print(\"----------------------------------\")\n",
    "print(RealVol.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgJ8e2Wj83Dw",
    "outputId": "500577e3-1429-4453-e92a-4e44bb211144"
   },
   "outputs": [],
   "source": [
    "print(sp_close_vol.tail(1))\n",
    "print(\"----------------------------------\")\n",
    "print(gld_price.tail(1))\n",
    "print(\"----------------------------------\")\n",
    "print(wti_price.tail(1))\n",
    "print(\"----------------------------------\")\n",
    "print(bill3m_rate.tail(1))\n",
    "print(\"----------------------------------\")\n",
    "print(bill1y_rate.tail(1))\n",
    "print(\"----------------------------------\")\n",
    "print(RealVol.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raa2utTK83Dw",
    "outputId": "1eb224fe-6636-4f46-e34b-210427fa0246"
   },
   "outputs": [],
   "source": [
    "print(sp_close_vol.info())\n",
    "print(\"----------------------------------\")\n",
    "print(gld_price.info())\n",
    "print(\"----------------------------------\")\n",
    "print(wti_price.info())\n",
    "print(\"----------------------------------\")\n",
    "print(bill3m_rate.info())\n",
    "print(\"----------------------------------\")\n",
    "print(bill1y_rate.info())\n",
    "print(\"----------------------------------\")\n",
    "print(RealVol.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weimlRhD83Dw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = list((sp_close_vol, gld_price, wti_price, bill3m_rate, bill1y_rate, RealVol))\n",
    "\n",
    "for df in data:\n",
    "    print(len(df))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdAJ0Qn083Dx"
   },
   "source": [
    "### Drop Calendar Holidays\n",
    "\n",
    "Check which dates are holidays and drop them from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_h1BGLvA83Dx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cal = calendar()\n",
    "holidays = cal.holidays(start=start_date, end=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB4T2pRN83Dx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_holidays(df, holidays):\n",
    "    df[\"Holiday\"] = df.index.isin(holidays)\n",
    "    df = df.drop(df[df[\"Holiday\"] == True].index)\n",
    "    df.drop(\"Holiday\", axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "sp = drop_holidays(sp_close_vol, holidays)\n",
    "gld_price = drop_holidays(gld_price, holidays)\n",
    "wti_price = drop_holidays(wti_price, holidays)\n",
    "bill3m_rate = drop_holidays(bill3m_rate, holidays)\n",
    "bill1y_rate = drop_holidays(bill1y_rate, holidays)\n",
    "RealVol = drop_holidays(RealVol, holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJY7vUX983Dx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = list((sp_close_vol, gld_price, wti_price, bill3m_rate, bill1y_rate, RealVol))\n",
    "for df in data:\n",
    "    print(len(df))\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxo5HnUa83Dx"
   },
   "source": [
    "## Merge Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRmoqwYY83Dx"
   },
   "source": [
    "### Merge bill rates and crude oil price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWbu6PpJ83Dx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_rates = pd.merge(\n",
    "    left=bill3m_rate, right=bill1y_rate, left_index=True, right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_rates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_rates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-NQURtI83D1",
    "outputId": "7b78fc2b-ff8a-41be-a031-c5609321d903",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(bill1y_rate))\n",
    "print(\"________________________________\")\n",
    "print(len(bill3m_rate))\n",
    "print(\"________________________________\")\n",
    "print(len(bill_rates))\n",
    "print(\"________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGAPrNyh83D1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_wti = bill_rates.merge(wti_price, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sucj6LoA83D1",
    "outputId": "3514eff4-7db3-487a-83ff-54c98b32e9d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_wti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "So5zH4tP83D2",
    "outputId": "bb365f8d-37a8-4b96-8523-e78e66a53841",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(bill_rates))\n",
    "print(\"________________________________\")\n",
    "print(len(wti_price))\n",
    "print(\"________________________________\")\n",
    "print(len(bill_wti))\n",
    "print(\"________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpYPg1xh83D2"
   },
   "source": [
    "### Merge with RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1JbqpZN83D2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_wti_rv = bill_wti.merge(RealVol, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oowNG4_N83D2",
    "outputId": "f7968cda-d868-47df-9639-71fb569f33ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "bill_wti_rv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3tP_OYK83D2",
    "outputId": "3ebfcd6c-9d20-466b-a093-8a1594cb8645",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(bill_wti))\n",
    "print(\"________________________________\")\n",
    "print(len(RealVol))\n",
    "print(\"________________________________\")\n",
    "print(len(bill_wti_rv))\n",
    "print(\"________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoE6DxFj83D2"
   },
   "source": [
    "### Merge with gold and obtain final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av6T_1vh83D2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = bill_wti_rv.merge(gld_price, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ1hqzj283D2",
    "outputId": "9ff881b8-87b9-401f-8fa7-3788970ec71f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9gD-EwP83D2",
    "outputId": "fdab7494-59e0-4d56-bb28-f2da48ed1698",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MTrIoOj83D3",
    "outputId": "ff4b90b1-5292-45b3-d9a3-f5a899d6338e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(bill_wti_rv))\n",
    "print(\"________________________________\")\n",
    "print(len(gld_price))\n",
    "print(\"________________________________\")\n",
    "print(len(df))\n",
    "print(\"________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFiXnxA083D3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spy = df.merge(sp_close_vol, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5FDN5TK83D3",
    "outputId": "03f2be5f-4a0b-4279-aebc-aa064f97ee86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4JJZ49t83D3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_spy.rename(\n",
    "    columns={\n",
    "        \"DCOILWTICO\": \"Oil\",\n",
    "        \"DGS3MO\": \"TBill3M\",\n",
    "        \"DGS1\": \"TBill1Y\",\n",
    "        \"CRV_over\": \"RV\",\n",
    "        \"GOLDPMGBD228NLBM\": \"Gold\",\n",
    "        \"Adj Close\": \"SP_close\",\n",
    "        \"Volume\": \"SP_volume\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFx0MhYP83D3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"weekday\"] = df.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDoow4HB83D3",
    "outputId": "d35ae27e-37b7-425a-c40f-951932c5f9a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis : A first look at the (time-series) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe where we have missing data in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be perfect, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(df[\"Gold\"], errors=\"coerce\").isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a function that tries to convert a value to float and returns a boolean result\n",
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Apply this function to each element of 'Gold' column\n",
    "mask = df[\"Gold\"].apply(is_float)\n",
    "\n",
    "# Print the rows in 'Gold' where the value could not be converted to float\n",
    "print(df[\"Gold\"][~mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Gold\"] = pd.to_numeric(df[\"Gold\"], errors=\"coerce\")\n",
    "df[\"Oil\"] = pd.to_numeric(df[\"Oil\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"TBill3M\", \"TBill1Y\"]] = df[[\"TBill3M\", \"TBill1Y\"]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"data/Rv_daily_lec4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot all the time series in the same figure to have a close look at their trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot is not easily interpretable because time series are on different scale! There's a quick solution though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T19:01:51.015454Z",
     "start_time": "2021-08-23T19:01:50.305622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.plot(logy=True, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Measures and Plots that helps with the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in data analytics is learning about your datasets' elementary characteristics. In terms of statistical science, we're talking about **Descriptive statistics** which includes: \n",
    "\n",
    "* measuring *sample mean, median, and mode*, as measures of central tendency; \n",
    "* learning about the probability distributions of relevant variables from the data set; \n",
    "* learning about dispersion by inspecting sample variances (or standard deviations), ranges, IQRs, and similar; \n",
    "* inspecting the data visually, of course. \n",
    "\n",
    "The latter procedures have come to be known as **Exploratory Data Analysis (EDA)** in the course of the second half of the 20th century; the term was popularised by the famous American statistician John Tukey (1915-2000), who published a book on the topic titled \"Exploratory Data Analysis\" in 1977.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of observation....but we don't have such observation for all the variables. We will consider only a piece of the dataframe for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"SP_close\"  # try also('displacement', 'horsepower', 'weight', 'acceleration')\n",
    "df[df[var] == df[var].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[var] == df[var].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box Plot** is the visual representation of the depicting groups of numerical data through their quartiles. Boxplot is also used to detect the outlier in the dataset. It captures the summary of the data efficiently with a simple box and whiskers and allows us to compare easily across groups. Boxplot summarizes sample data using 25th, 50th, and 75th percentiles. These percentiles are also known as the lower, median, and upper quartiles.\n",
    "\n",
    "A box plot consists of 5 things:\n",
    " \n",
    "* First Quartile or 25%\n",
    "* Median (Second Quartile) or 50%\n",
    "* Third Quartile or 75%\n",
    "* IQR (Inter Quantile Range)\n",
    "* Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df[\"SP_close\"].quantile(0.25)  # Compute the quantile\n",
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SP_close\"].quantile([0.25, 0.50, 0.75])  # A sequence of quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter quantile Range\n",
    "Q3 = df[\"SP_close\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot using pandas or seaborn\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"float64\":\n",
    "        fig, ax = fig, ax = plt.subplots(figsize=(8, 3))\n",
    "        sns.boxplot(x=df[column], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to:\n",
    "\n",
    "* Upper whisker = $min(max(x), Q3 + 1.5*IQR)$\n",
    "* Lower whisker = $max(min(x), Q1 - 1.5*IQR)$ \n",
    "\n",
    "Outlier points are those past the end of the whiskers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and Kernel density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram using Pandas\n",
    "df[\"SP_close\"].plot.hist(density=1, bins=100)  # choose number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram using Pandas\n",
    "df[\"TBill1Y\"].plot.hist(density=1, bins=100)  # choose number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel density estimation\n",
    "df[\"SP_close\"].plot.density(bw_method=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram using Pandas\n",
    "df[\"TBill1Y\"].plot.density(bw_method=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[\"Return_close\"])  # using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df[\"Return_close\"].skew())\n",
    "print(\"Kurtosis: %f\" % df[\"Return_close\"].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness and kurtosis are two important statistical concepts that describe the shape of a distribution.\n",
    "\n",
    "Skewness refers to a distribution's symmetry, or more precisely, the lack thereof. A skewness of 0 means the distribution is perfectly symmetrical around the mean, while a negative skew indicates that the left tail is longer or fatter than the right. In your case, a skewness of -3.093178 means that the 'Return_close' distribution is heavily skewed to the left.\n",
    "\n",
    "Kurtosis, on the other hand, refers to the \"tailedness\" of the distribution. A kurtosis of 3 (or 0, depending on the definition used) is expected from a normal distribution. If the kurtosis is higher than 3 (or 0), the distribution is said to be leptokurtic, which means it has heavier tails or a sharper peak than the normal distribution. The value of 94.653784 for kurtosis in your case is significantly larger than 3, suggesting a leptokurtic distribution with extremely heavy tails and a sharp peak. This indicates a high probability of extreme values or outliers in your 'Return_close' distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the majority of machine learning models, but also standard econometrics models, it is often convenient to scale the data to the same range of [0,1]\n",
    "\n",
    "\n",
    "In general, learning algorithms benefit from the standardization of the data set.\n",
    "If some outliers are present in the set, robust scalers or transformers are more appropriate. The `preprocessing` module of the `sklearn` package provides several standard utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the estimators we will use. In the following classes, we will see a different use of these scalers. \n",
    "\n",
    "**REMARK** : *Standardization of datasets is a common requirement for many machine learning estimators that might misbehave if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.*\n",
    "\n",
    "In practice, we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "**The reasons are the following**: many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the $L_1$ and $L_2$ regularizers of linear models) assume that all features are centered around zero and have variance in the same order. Suppose a feature has a larger order of magnitude variance than others. In that case, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "\n",
    "Before using `sklearn`, we can define a function `scale` to look at the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(a):\n",
    "    b = (a - a.mean()) / a.std()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale = (\n",
    "    df.copy()\n",
    ")  # remember that otherwise data_scale will be another name for the same object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale = df_scale.apply(standard_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our data is now scaled to resemble a Gaussian distribution with unit variance. This will help us visualize data better and help the model to ingest them. We used a copy of the original data-set for this as we will use the original dataset later when we build regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other possible Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler : standardize column with mean and standard deviation\n",
    "data_scaled = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "data_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there are many outliers, we can use a robust routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_robust = pd.DataFrame(RobustScaler().fit_transform(df), columns=df.columns)\n",
    "data_robust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"RV\"\n",
    "data_plt = pd.concat([np.sign(df[\"Return_close\"]), df[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x=\"Return_close\", y=var, data=data_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Return_close\"].iloc[np.where(np.sign(df[\"Return_close\"]) == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMINDER** Given two random variables, $X$ and $Y$, their (sample) covariance is given by:\n",
    "$$Cov(X,Y)= \\mathbb{E}\\left[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) \\right]= \\frac{1}{N-1}\\sum_{i=1}^N (X_i-\\hat{X})(Y_i-\\hat{Y}) $$\n",
    "and the Pearson correlation is just the covariance of standardized variables:\n",
    "$$ \\rho(X,Y)=\\frac{Cov(X,Y)}{\\sigma(X)\\sigma(Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var = \"RV\"\n",
    "plot = sns.lmplot(y=var, x=\"Return_close\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data[['mpg',var]].corr()   #compute Pearson correlation\n",
    "np.corrcoef([df[\"Return_close\"], df[\"RV\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pearsonr(df[\"Return_close\"], df[\"RV\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust correlation (Spearman correlation)\n",
    "In presence of outliers one can use Robust Statistics: the Spearman correlation is defined as \n",
    "$$\\hat{\\rho}(X,Y)= \\rho(Rank(X),Rank(Y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot = sns.lmplot(y=var, x=\"Return_close\", data=df.rank())\n",
    "# Equal values are assigned a rank that is the average of the ranks of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[[\"RV\", \"Return_close\"]].corr(method=\"spearman\")  # compute Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Colormap choices](https://matplotlib.org/stable/tutorials/colors/colormaps.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=0.3,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests\n",
    "\n",
    "In finance we often work with time series. Before starting any modeling task, we need to understand if these time series respect some properties.\n",
    "\n",
    "\n",
    "### Dickey-Fuller Test for Stationarity\n",
    "\n",
    "The Dickey-Fuller test, specifically the Augmented Dickey-Fuller (ADF) test, is a statistical procedure used to test whether a time series is stationary. A time series is said to be stationary if its statistical properties such as mean and variance remain constant over time. The hypothesis behind the Dickey-Fuller test can be detailed as follows:\n",
    "\n",
    "#### Null Hypothesis (H0):\n",
    "The null hypothesis of the test is that the time series is non-stationary, implying that it has a unit root and the time series follows a random walk model. Mathematically, it can be represented as:\n",
    "\n",
    "\n",
    "$$[ H_0: \\delta = 0 \\, (\\text{Unit Root Present}) ]$$\n",
    "\n",
    "#### Alternative Hypothesis (H1):\n",
    "The alternative hypothesis is that the time series is stationary, meaning it does not have a unit root and the time-dependent structure in the series is stable over time. Mathematically, it can be represented as:\n",
    "\n",
    "$$[ H_1: \\delta < 0 \\, (\\text{No Unit Root}) ]$$\n",
    "\n",
    "#### Test Statistic:\n",
    "The test statistic is calculated based on the coefficients of an autoregressive model fitted to the data. The formula for the test statistic is given by:\n",
    "\n",
    "$$[ \\text{ADF Statistic} = \\frac{(\\hat{\\delta} - 0)}{\\text{Standard Error of } \\hat{\\delta}} ]$$\n",
    "\n",
    "#### Critical Values:\n",
    "The test statistic is then compared with critical values at different significance levels (usually 1%, 5%, and 10%) to decide whether to reject or fail to reject the null hypothesis. If the test statistic is less than the critical value, we reject the null hypothesis in favor of the alternative hypothesis, indicating stationarity.\n",
    "\n",
    "#### Implementation:\n",
    "In practice, the test is implemented by estimating the following regression model:\n",
    "\n",
    "$$[ \\Delta Y_t = \\alpha + \\beta t + \\gamma Y_{t-1} + \\delta_1 \\Delta Y_{t-1} + \\delta_2 \\Delta Y_{t-2} + \\ldots + \\delta_p \\Delta Y_{t-p} + \\varepsilon_t ]$$\n",
    "\n",
    "Where:\n",
    "- $ΔY_t$: The difference in the series at time $t$\n",
    "- $α$: Constant term\n",
    "- $βt$: Trend component\n",
    "- $γ$: Coefficient of the lagged level of the series\n",
    "- $δ_i$: Coefficients of the lagged differences of the series\n",
    "- $ε_t$: Error term at time $t$\n",
    "\n",
    "\n",
    "The Dickey-Fuller test is a powerful tool for analyzing the stationarity of time series data, which is a crucial assumption in many time series modeling techniques, including ARIMA and GARCH models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df[\"Return_close\"].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T19:01:58.358159Z",
     "start_time": "2021-08-23T19:01:52.850695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns from df\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# ADF statistic to check stationarity\n",
    "for col in df_numeric.columns:\n",
    "    timeseries = df_numeric[col].dropna()\n",
    "    result = adfuller(timeseries)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ax = timeseries.plot(secondary_y=False, logy=False)\n",
    "\n",
    "    print(\n",
    "        f\"Testing {col} from {timeseries.index[0]:%Y-%m-%d} to {timeseries.index[-1]:%Y-%m-%d} for Stationarity\"\n",
    "    )\n",
    "    print(f\"ADF Statistic: {result[0]:.3f}\")\n",
    "    print(f\"p-value: {result[1]:.3E}\")\n",
    "\n",
    "    if result[0] > result[4][\"5%\"]:\n",
    "        conclusion = (\n",
    "            f\"Failed to Reject H_0 at 5% -> {col} Time Series is Non-Stationary\"\n",
    "        )\n",
    "    else:\n",
    "        conclusion = f\"Reject H_0 at at 5% -> {col} Time Series is Stationary\"\n",
    "    print(conclusion)\n",
    "\n",
    "    ax.text(\n",
    "        x=timeseries.index[-1],\n",
    "        y=timeseries.max(),\n",
    "        s=f\"Testing {col}\\nADF Statistic: {result[0]:.3f}\\np-value: {result[1]:.3E}\\n\"\n",
    "        + conclusion,\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5, boxstyle=\"round,pad=0.5\"),\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differentiation\n",
    "\n",
    "If we calculate the differences of a time series (often enough) we can obtain a stationary time series but we throw away information about the absolute level and recent development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T19:01:58.836135Z",
     "start_time": "2021-08-23T19:01:58.360362Z"
    }
   },
   "outputs": [],
   "source": [
    "# SP_close\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"SP_close\"].plot(label=\"SP_close\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"SP Close\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"SP Close Value\")\n",
    "\n",
    "# Differences in SP_close\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"SP_close\"].diff().plot(alpha=0.5, label=\"Differences\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Differences in SP Close\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Difference\")\n",
    "\n",
    "\n",
    "# Return_close\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"Return_close\"].plot(alpha=0.5, label=\"Percent Change\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Percent Change (Return Close)\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Return Close\")\n",
    "\n",
    "# Log Differences\n",
    "plt.figure(figsize=(8, 4))\n",
    "np.log(df[\"SP_close\"]).diff().plot(alpha=0.5, label=\"Log Differences\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Log Differences in SP Close\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Log Difference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractional Differentiation of Financial Time Series\n",
    "\n",
    "Fractional differentiation is a mathematical technique used in the analysis of time series data, particularly in the field of finance. This technique allows for the transformation of non-stationary time series data into stationary, while maintaining memory properties, which is often lost in the process of regular differentiation.\n",
    "\n",
    "In financial time series analysis, fractional differentiation can be a powerful tool for modeling and forecasting, as it helps in retaining as much information as possible about the original series, which can be crucial for predicting future movements.\n",
    "\n",
    "The process of fractional differentiation is mathematically complex, involving the application of fractional calculus. The fractional differencing operator is defined as:\n",
    "\n",
    "$$[\n",
    "D^{d}X_t = \\sum_{k=0}^{\\infty} \\binom{d}{k} (-1)^k X_{t-k}\n",
    "]$$\n",
    "\n",
    "Where:\n",
    "- $D^{d}$: Fractional differencing operator\n",
    "- $X_t$: Time series data at time $t$\n",
    "- $d$: Order of differentiation, which is a fractional number\n",
    "- $k$: Lag operator\n",
    "\n",
    "The choice of the differentiation order $d$ is critical, as it determines the balance between removing noise and retaining memory in the series.\n",
    "\n",
    "#### References\n",
    "1. Hosking, J. R. M. (1981). Fractional differencing. Biometrika, 68(1), 165-176.\n",
    "2. Granger, C. W. J., & Joyeux, R. (1980). An introduction to long-memory time series models and fractional differencing. Journal of Time Series Analysis, 1(1), 15-29.\n",
    "3. Hurst, H. E. (1951). Long-term storage capacity of reservoirs. Transactions of the American Society of Civil Engineers, 116, 770-799.\n",
    "4. Walasek, R., & Gajda, J. (Year of Publication). Fractional differentiation and its use in machine learning.  International Journal of Advances in Engineering Sciences and Applied Mathematics 13.2-3 (2021): 270-277.\n",
    "\n",
    "\n",
    "By understanding and applying fractional differentiation, financial analysts and researchers can develop more accurate and informative models for financial time series analysis, enhancing the predictive power and reliability of their analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_difference(series, d, lag_cutoff=1e-5):\n",
    "    \"\"\"\n",
    "    Apply fractional differentiation to a time series.\n",
    "    series: pandas Series\n",
    "    d: Fractional order of differentiation\n",
    "    lag_cutoff: Threshold to truncate weights\n",
    "    \"\"\"\n",
    "    weights = [1.0]  # Initialize the weights\n",
    "    for k in range(1, len(series)):\n",
    "        weight = -weights[-1] * (d - k + 1) / k\n",
    "        if abs(weight) < lag_cutoff:  # Stop if weight is too small\n",
    "            break\n",
    "        weights.append(weight)\n",
    "    weights = np.array(weights[::-1])  # Reverse weights for convolution\n",
    "\n",
    "    # Apply fractional differencing\n",
    "    diff_series = np.convolve(series, weights, mode=\"valid\")\n",
    "    return pd.Series(diff_series, index=series.index[len(weights) - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for differentiation\n",
    "orders = [0.2, 0.5, 0.8]\n",
    "alphas = orders[::-1]# Fractional differentiation orders\n",
    "\n",
    "# Generate fractionally differentiated series\n",
    "frac_diff_series = {f\"d={d}\": fractional_difference(df[\"SP_close\"], d) for d in orders}\n",
    "\n",
    "# Plot the original series\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df[\"SP_close\"], label=\"Original SP_close\", linewidth=2, color=\"blue\")\n",
    "plt.title(\"Original SP_close Series\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"SP_close\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "# Plot the fractionally differentiated series\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i,(d, series) in enumerate(frac_diff_series.items()):\n",
    "    plt.plot(series, label=f\"Fractional Diff ({d})\", linewidth=2, alpha=alphas[i])\n",
    "plt.title(\"Fractionally Differentiated SP_close Series\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Value\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Are Observations Lost in Fractional Differentiation?\n",
    "\n",
    "Fractional differentiation requires a weighted sum of past values in the series. The process truncates these weights when they become negligibly small, which leads to the loss of initial observations.\n",
    "\n",
    "The number of observations lost depends on:\n",
    "- The fractional order ($d$): Lower $d$ retains memory longer, requiring more past values.\n",
    "- The cutoff threshold: Smaller thresholds retain more terms but lose more observations.\n",
    "\n",
    "To reduce data loss, you can adjust the cutoff threshold or use higher $d$ values, though this may affect the balance between memory retention and stationarity.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fintech_lectures",
   "language": "python",
   "name": "fintech_lectures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "197px",
    "width": "656px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
