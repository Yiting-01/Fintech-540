{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ca7524-ac68-4c5c-b245-3540b31a9968",
   "metadata": {},
   "source": [
    "# Leveraging on tree-based methods\n",
    "\n",
    "This lecture will focus on techniques to create **ensembles** of different tree models in order to enhance performance. Remember that we defined regression/classification trees as weak learners that are just slightly better than a random guess in predicting/classifying data.\n",
    "\n",
    "**REMARK** Note that when we refer to an ensemble, i.e., a blend of different models to produce a prediction/classification, that notion is very general and doesn't encompass just a specific model. We present ensemble under the umbrella of the tree-based method since they are known to perform well on several tasks, although the notion of ensemble is very general.\n",
    "\n",
    "**TAKEAWAY** You could construct ensembles by mixing different types of models (a regression tree and a neural network), but we try to keep it simple here.\n",
    "\n",
    "[Scikit-learn doc](https://scikit-learn.org/stable/modules/ensemble.html) has a good tutorial on ensemble methods that you can use as a reference for this lecture.\n",
    "\n",
    "We first start with some theory, then we will move towards a dataset including credit card data, and we will apply both techniques to this same problem.\n",
    "\n",
    "## Bagging \n",
    "\n",
    "**Bagging**, also called bootstrap aggregation, is a technique for reducing the variance of the output of a model. *Bagging works especially well for high-variance, low-bias procedures, such as trees*. **For regression**, we simply fit the same regression tree many times to bootstrap sampled (kind of random sampling with replacement) versions of the training data and average the result. **For classification**, a committee of trees each cast a vote for the predicted class.\n",
    "\n",
    "**Boosting**, which we are going to explore later, was initially proposed as a committee method as well, although, unlike bagging, the committee of weak learners evolves over time, and the members cast a weighted vote. Boosting appears to dominate bagging on most problems and has become the preferred choice.\n",
    "\n",
    "**Random forests** is a modification of bagging that builds a large collection of **de-correlated trees** and then **averages** them. On many problems, the performance of random forests is very similar to boosting, and they are simpler to train and tune. As a consequence, random forests are popular and are implemented in a variety of packages.\n",
    "\n",
    "\n",
    "### Random Forests\n",
    "The idea is to train many approximately unbiased models and, hence, reduce the variance by averaging their noise. Trees are ideal candidates for bagging since they can capture complex interaction structures in the data and, if grown sufficiently deep, have relatively low bias. \n",
    "\n",
    "However, being noisy, they greatly benefit from averaging. Moreover, since each tree generated in bagging is identically distributed (i.d.), the expectation of an average of $B$ such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual (bootstrap) trees, and the only hope of improvement is through variance reduction. *This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias and hence are not i.d.*\n",
    "\n",
    "An average of $B$ i.i.d. random variables, each with variance $\\sigma^{2}$, has variance $\\frac{1}{B} \\sigma^{2}$. If the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation $\\rho$, the variance of the average is \n",
    "$$\n",
    "\\rho \\sigma^{2}+\\frac{1-\\rho}{B} \\sigma^{2}\n",
    "$$\n",
    "As $B$ increases, the second term disappears, but the first remains, and hence, the size of the correlation of pairs of bagged trees limits the benefits of averaging. \n",
    "\n",
    "The idea in random forests is to improve the variance reduction of bagging by reducing the correlation between the trees, without increasing the variance too much. This is achieved in the tree-growing process through random selection of the input variables.\n",
    "\n",
    "<img src=\"images/rf_algo.png\" width=\"600\">\n",
    "\n",
    "\n",
    "When growing a tree on a bootstrapped dataset:\n",
    "\n",
    "Before each split, select $m \\leq p$ of the input variables at random as candidates for splitting. Typically, values for $m$ are $\\sqrt{p}$ or even as low as 1.\n",
    "\n",
    "\n",
    "After $B$ such trees $\\left\\{T\\left(x; \\Theta_{b}\\right)\\right\\}_{1}^{B}$ are grown, the random forest (regression) predictor is\n",
    "$$\n",
    "\\hat{f}_{\\mathrm{rf}}^{B}(x)=\\frac{1}{B} \\sum_{b=1}^{B} T\\left(x ; \\Theta_{b}\\right) .\n",
    "$$\n",
    "We observe that $\\Theta_{b}$ characterizes the $b$-th random forest tree in terms of split variables, cutpoints at each node, and terminal-node values. Intuitively, reducing $m$ will reduce the correlation between any pair of trees in the ensemble.\n",
    "\n",
    "**Not all estimators can be improved by shaking up the data like this. It seems that highly nonlinear estimators, such as trees, benefit the most.** \n",
    "*For bootstrapped trees, $\\rho$ is typically small $(0.05$ or lower is typical), while $\\sigma^{2}$ is not much larger than the variance for the original tree. On the other hand, bagging does not change linear estimates, such as the sample mean (hence its variance either); the pairwise correlation between bootstrapped means is about $50 \\%$.*\n",
    "\n",
    "\n",
    "\n",
    "**Random forests do remarkably well, with very little tuning required.**\n",
    "\n",
    "\n",
    "\n",
    "When used for classification, a random forest obtains a class vote from each tree and then classifies using a majority vote (as a committee). When used for regression, the predictions from each tree at a target point $x$ are simply averaged. \n",
    "\n",
    "In addition, consider that:\n",
    "- For classification, the default value for $m$ is $\\lfloor\\sqrt{p}\\rfloor$ and the minimum node size is one.\n",
    "- For regression, the default value for $m$ is $\\lfloor p / 3\\rfloor$ and the minimum node size is five.\n",
    "\n",
    "**In practice, the best values for these parameters will depend on the problem**, and they should be treated as tuning parameters. \n",
    "\n",
    "\n",
    "When the number of variables is large, but the fraction of relevant variables is small, random forests are likely to perform poorly with small $m$. At each split, the chance can be small that the relevant variables will be selected. When the number of relevant variables increases, the performance of random forests is surprisingly robust to an increase in the number of noise variables (look at the example provided in the book). \n",
    "\n",
    "\n",
    "There is the claim that random forests \"cannot overfit\" the data. It is certainly true that increasing $B$ does not cause the random forest sequence to overfit; like bagging, the random forest estimate approximates the expectation.\n",
    "$$\n",
    "\\hat{f}_{\\mathrm{rf}}(x)=\\mathrm{E}_{\\Theta} T(x ; \\Theta)=\\lim _{B \\rightarrow \\infty} \\hat{f}(x)_{\\mathrm{rf}}^{B}\n",
    "$$\n",
    "with an average over $B$ realizations of $\\Theta$. The distribution of $\\Theta$ here is conditional on the training data. However, this limit can overfit the data; the average of fully grown trees can result in too rich a model and incur unnecessary variance. \n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting was originally designed for classification problems, but as will be seen in this chapter, it can profitably be extended to regression as well. \n",
    "\n",
    "The most popular boosting algorithm is **AdaBoost**. Consider a two-class problem, with the output variable coded as $Y \\in\\{-1,1\\}$. Given a vector of predictor variables $X$, a classifier $G(X)$ produces a prediction taking one of the two values $\\{-1,1\\}$. The error rate on the training sample is\n",
    "$$\n",
    "\\overline{\\mathrm{err}}=\\frac{1}{N} \\sum_{i=1}^{N} I\\left(y_{i} \\neq G\\left(x_{i}\\right)\\right),\n",
    "$$\n",
    "and the expected error rate on future predictions is $\\mathrm{E}_{X Y} I(Y \\neq G(X))$.\n",
    "A weak classifier is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak classifiers $G_{m}(x), m=1,2, \\ldots, M$.\n",
    "\n",
    "<img src=\"images/ab_algo.png\" width=\"600\">\n",
    "\n",
    "The predictions from all of them are then combined through a weighted majority vote to produce the final prediction:\n",
    "$$\n",
    "G(x)=\\operatorname{sign}\\left(\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)\\right)\n",
    "$$\n",
    "Here $\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{M}$ are computed by the boosting algorithm and weight the contribution of each respective $G_{m}(x)$. Their effect is to give a higher influence to the more accurate classifiers in the sequence.\n",
    "\n",
    "The data modifications at each boosting step consist of applying weights $w_{1}, w_{2}, \\ldots, w_{N}$ to each of the training observations $\\left(x_{i}, y_{i}\\right), i=1,2, \\ldots, N$. Initially, all of the weights are set to $w_{i}=1 / N$ so that the first step simply trains the classifier on the data in the usual manner. For each successive iteration $m=2,3, \\ldots, M$, the observation weights are individually modified, and the classification algorithm is reapplied to the weighted observations. At step $m$, those observations that were misclassified by the classifier $G_{m-1}(x)$ induced at the previous step have their weights increased, whereas the weights are decreased for those that were classified correctly. Thus, as iterations proceed, observations that are difficult to classify correctly receive ever-increasing influence. Each successive classifier is thereby forced to concentrate on those training observations that are missed by previous ones in the sequence.\n",
    "\n",
    "<img src=\"images/ab_algo_code.png\" width=\"600\">\n",
    "\n",
    "The details of the AdaBoost algorithm are shown in the algorithm above. The current classifier $G_{m}(x)$ is induced on the weighted observations at line $2 \\mathrm{a}$. The resulting weighted error rate is computed at line $2 b$. Line $2 c$ calculates the weight $\\alpha_{m}$ given to $G_{m}(x)$ in producing the final classifier $G(x)$ (line $3)$. The individual weights of each of the observations are updated for the next iteration at line $2 \\mathrm{~d}$. Observations misclassified by $G_{m}(x)$ have their weights scaled by a factor $\\exp \\left(\\alpha_{m}\\right)$, increasing their relative influence for inducing the next classifier $G_{m+1}(x)$ in the sequence.\n",
    "\n",
    "*The presented algorithm is known as \"Discrete AdaBoost\" because the base classifier $G_{m}(x)$ returns a discrete class label.* If the base classifier instead returns a real-valued prediction (e.g., a probability mapped to the interval $[-1,1])$, AdaBoost can be modified appropriately and called \"Real AdaBoost.\"\n",
    "\n",
    "You can have a look at the toy example in the book to have an idea about the power of boosting techniques in improving the performances of weak learners.\n",
    "\n",
    "**Why is boosting  so powerful?**\n",
    "\n",
    "It is a way of fitting an additive expansion in a set of elementary \"basis\" functions. Here the basis functions are the individual classifiers $G_{m}(x) \\in\\{-1,1\\}$. More generally, basis function expansions take the form\n",
    "$$\n",
    "f(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right),\n",
    "$$\n",
    "where $\\beta_{m}, m=1,2, \\ldots, M$ are the expansion coefficients, and $b(x ; \\gamma) \\in \\mathbb{R}$. are usually simple functions of the multivariate argument $x$, characterized by a set of parameters $\\gamma$. We discuss basis expansions in some detail in Chapter $5 .$\n",
    "\n",
    "Additive expansions like this are at the heart of many of the learning techniques covered in this book:\n",
    "- In single-hidden-layer neural networks, $b(x ; \\gamma)=\\sigma\\left(\\gamma_{0}+\\right.$ $\\left.\\gamma_{1}^{T} x\\right)$, where $\\sigma(t)=1 /\\left(1+e^{-t}\\right)$ is the sigmoid function, and $\\gamma$ parameterizes a linear combination of the input variables.\n",
    "- In signal processing, wavelets are a popular choice with $\\gamma$ parameterizing the location and scale shifts of a \"mother\" wavelet.\n",
    "- Multivariate adaptive regression splines use truncated power spline basis functions where $\\gamma$ parameterizes the variables and values for the knots.\n",
    "\n",
    "- For trees, $\\gamma$ parameterizes the split variables and split points at the internal nodes and the predictions at the terminal nodes.\n",
    "\n",
    "Typically, these models are fit by minimizing a loss function averaged over the training data, such as the squared-error or a likelihood-based loss function,\n",
    "$$\n",
    "\\min _{\\left\\{\\beta_{m}, \\gamma_{m}\\right\\}_{1}^{M}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right) .\n",
    "$$\n",
    "For many loss functions $L(y, f(x))$ and/or basis functions $b(x; \\gamma)$, this requires computationally intensive numerical optimization techniques. However, a simple alternative often can be found when it is feasible to rapidly solve the subproblem of fitting just a single basis function,\n",
    "$$\n",
    "\\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, \\beta b\\left(x_{i} ; \\gamma\\right)\\right) .\n",
    "$$\n",
    "\n",
    "There are other variants of Boosting algorithms for trees that you may find implemented in `scikit-learn.` There are obviously variations of the same approach which are more suitable for certain problems. The key concept that you should be familiar with at this point is the difference between **bagging** and **boosting**, knowing that not necessarily one outperforms the other. This is even more true in the world of finance.\n",
    "\n",
    "### AdaBoost and Gradient Boosting: A Comparison\n",
    "\n",
    "Both AdaBoost and Gradient Boosting are ensemble methods that aim to improve the predictive performance of weak learners by combining them into a single strong learner. However, there are significant differences in their approaches and underlying principles. Below, I outline their similarities and differences, specifically in the context of how they are implemented in Scikit-learn and XGBoost.\n",
    "\n",
    "**AdaBoost**\n",
    "\n",
    "\n",
    "*AdaBoost (Adaptive Boosting)* starts by fitting a base learner—often a decision tree with a single split, also known as a \"stump\"—to the original dataset. In each subsequent iteration, it adjusts the weights of the training instances according to the errors made in the previous iteration. The base learner is then refitted to this reweighted data.\n",
    "\n",
    "*Weighting Errors*\n",
    "In classification, misclassified samples gain higher weights to gain more attention from future base learners. In regression, instances with larger errors have their weights increased.\n",
    "\n",
    "*Combination*\n",
    "The final prediction is derived from a weighted majority vote in classification tasks, and from a weighted sum in regression tasks.\n",
    "\n",
    "*Model Complexity*\n",
    "Typically employs simple base learners, like decision stumps.\n",
    "\n",
    "*Objective Function*\n",
    "Aims to minimize the weighted error rate, which could pertain to classification error or some continuous error in regression.\n",
    "\n",
    "*Scikit-learn Implementation*\n",
    "Available as `AdaBoostClassifier` for classification and `AdaBoostRegressor` for regression.\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Boosting**\n",
    "\n",
    "*Gradient Boosting* also initiates by fitting a base learner to the original dataset. It then iteratively adds new models that focus on correcting the residuals (in regression) or the gradients of the loss function (in classification) for the combined ensemble of existing models.\n",
    "\n",
    "*Gradient Descent*\n",
    "Rather than altering instance weights as in AdaBoost, Gradient Boosting fits the new model to the residuals or gradients, performing a form of gradient descent in the function space.\n",
    "\n",
    "*Combination*\n",
    "The final prediction is a weighted sum of the base learners' predictions, whether for classification probabilities or regression outputs.\n",
    "\n",
    "*Model Complexity*\n",
    "Allows for more complex base learners, such as larger decision trees.\n",
    "\n",
    "*Objective Function*\n",
    "Typically optimized for a differentiable loss function, offering flexibility for different types of problems, be it classification or regression.\n",
    "\n",
    "*Scikit-learn Implementation*\n",
    "Implemented as `GradientBoostingClassifier` for classification and `GradientBoostingRegressor` for regression.\n",
    "\n",
    "*XGBoost* is an optimized distributed gradient boosting library that is designed for high efficiency, flexibility, and portability. It often outperforms scikit-learn's Gradient Boosting in both speed and adaptability to various problem types.\n",
    "\n",
    "\n",
    "**GradientBoosting vs. XGBoost**\n",
    "\n",
    "There are some key differences between those two:\n",
    "- **Algorithmic Enhancements**: XGBoost incorporates several algorithmic enhancements for tree pruning, regularization, and handling of missing data, among other things.\n",
    "- **Optimization**: XGBoost is designed for speed and performance. It is engineered to be distributed and can be parallelized across clusters, which scikit-learn's Gradient Boosting is not natively designed for.\n",
    "- **Flexibility**: XGBoost is generally more flexible, allowing for custom objective functions and evaluation criteria, among other features.\n",
    "- **Regularization**: XGBoost has an additional regularization term in the objective function, which helps to reduce overfitting. Scikit-learn's implementation does not have this feature by default.\n",
    "- **Handling Missing Data**: XGBoost has a built-in routine to handle missing data, while in scikit-learn, you would typically need to handle missing data during the preprocessing stage.\n",
    "- **Early Stopping**: XGBoost allows for early stopping during the training process, which is not available by default in scikit-learn's GradientBoosting implementation.\n",
    "- **Support for Various Types of Problems**: XGBoost can be used for regression, classification, ranking, and user-defined prediction problems. Scikit-learn's Gradient Boosting is not as flexible for different kinds of specialized problems.\n",
    "\n",
    "\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- **Commonality**: Both are boosting algorithms that combine multiple weak learners to create a strong learner.\n",
    "\n",
    "- **Key Difference**: AdaBoost focuses on training instances that are hard to predict, whereas Gradient Boosting focuses on correcting the errors of the combined ensemble.\n",
    "\n",
    "- **Flexibility**: Gradient Boosting is generally more flexible and can be optimized for a variety of loss functions. This makes it applicable to a wider array of problems compared to AdaBoost.\n",
    "\n",
    "By understanding these differences and similarities, you can make a more informed choice between AdaBoost, Gradient Boosting in scikit-learn, and XGBoost based on your specific requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518b7e8-e88f-47fe-a370-94b84b1219ba",
   "metadata": {},
   "source": [
    "# Detecting Credit Card Default with Tree Ensembles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4fbe9-b0ad-434a-9fb9-a5d144b3d32e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-29T15:24:55.068103Z",
     "start_time": "2020-01-29T15:24:55.064292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import warnings\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import missingno\n",
    "\n",
    "# Preprocessing and Feature Engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "\n",
    "# Modeling and Evaluation\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    cross_val_score, \n",
    "    cross_validate, \n",
    "    StratifiedKFold\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Graphing and Visualization Tools\n",
    "import pydotplus\n",
    "\n",
    "# Global Settings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdcc0b-7c03-471b-9919-7359520957c8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6b218-f1c2-46bb-9081-5bb56d410176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/credit_card_default.xls\", skiprows=1, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e4c09-37be-4cf3-a1a4-40112c8b1b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249107a-8d2c-44fe-bbbe-aaddf54632e3",
   "metadata": {},
   "source": [
    "Get summary statistics for numeric variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebce60-684e-424e-8f82-d9ff871e7bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:54:30.435731Z",
     "start_time": "2020-01-22T17:54:30.384308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe().transpose().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd52369-f425-455b-af61-8886bd62ae8a",
   "metadata": {},
   "source": [
    "Plot the distribution of age and split it by gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db60eca-0744-4205-a769-82875ddb4386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:54:50.012891Z",
     "start_time": "2020-01-22T17:54:45.537942Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Violin Plot using Seaborn\n",
    "sns.violinplot(x=\"SEX\", y=\"AGE\", data=df, inner=\"quartile\")\n",
    "plt.title(\"Distribution of Age by Gender\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e59a3-9814-4445-ae76-52ca64dbb4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe7a43-8025-4786-a4d1-130ac23a9d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['AGE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec271c-7433-4bf9-934c-78f5d1f8402f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:54:55.216865Z",
     "start_time": "2020-01-22T17:54:51.549054Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a histogram using Seaborn's displot\n",
    "ax = sns.displot(\n",
    "    data=df,\n",
    "    # bins=20,\n",
    "    x='AGE',\n",
    "    kind='hist',\n",
    "    hue='SEX',\n",
    "    palette={1: \"blue\", 2: \"red\"} \n",
    ")\n",
    "ax.set(title='Distribution of Age by Gender', xlabel='Age', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdb1f8-471e-4cc5-a027-c765137759ee",
   "metadata": {},
   "source": [
    "We notice some spikes appearing every ~10 years and the reason for this is the binning. Below, we create the same histogram using `sns.countplot`. By doing so, each value of age has a separate bin and we can inspect the plot in detail. There are no such spikes in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330ac6d-fb31-4c62-86a0-ea5acd6b2649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:55:08.261624Z",
     "start_time": "2020-01-22T17:55:04.698261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_ = sns.countplot(x=df['AGE'], color=\"blue\")\n",
    "\n",
    "for ind, label in enumerate(plot_.get_xticklabels()):\n",
    "    if int(float(label.get_text())) % 10 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb40dbd-b82d-42b5-84a9-02804bfc1067",
   "metadata": {},
   "source": [
    "Plot a `pairplot` of selected variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0041a-f5b1-4389-b9dd-cdd221510880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d275828-54bb-48a6-a6a9-ac90c9797d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:55:35.820357Z",
     "start_time": "2020-01-22T17:55:16.118022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(df[[\"AGE\", \"LIMIT_BAL\", \"PAY_2\"]])\n",
    "pair_plot.fig.suptitle(\"Pairplot of selected variables\", y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375994c-1c0b-40de-9229-6e3b29c2e047",
   "metadata": {},
   "source": [
    "Additionally, we can separate the genders by specifying the `hue` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6e854-b7d2-45cd-8bd8-515010bf151c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T11:55:24.289688Z",
     "start_time": "2020-01-28T11:55:24.287495Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(\n",
    "    data=df, \n",
    "    x_vars = [\"AGE\", \"LIMIT_BAL\", \"PAY_2\"],\n",
    "    y_vars = [\"AGE\", \"LIMIT_BAL\", \"PAY_2\"],\n",
    "    hue=\"SEX\",\n",
    "    palette={1: \"blue\", 2: \"red\"} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf381b-350c-472b-9202-36dee565c0aa",
   "metadata": {},
   "source": [
    "Plot the correlation heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03603994-7ccf-4f4e-b105-a4ce5c1472b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:56:07.556117Z",
     "start_time": "2020-01-22T17:56:07.549240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(corr_mat):\n",
    "    \"\"\"\n",
    "    Function for plotting the correlation heatmap. It masks the irrelevant fields.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corr_mat : pd.DataFrame\n",
    "        Correlation matrix of the features.\n",
    "    \"\"\"\n",
    "\n",
    "    # temporarily change style\n",
    "    sns.set(style=\"white\")\n",
    "    # mask the upper triangle\n",
    "    mask = np.zeros_like(corr_mat, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # set up the matplotlib figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    # set up custom diverging colormap\n",
    "    cmap = sns.diverging_palette(240, 10, n=9, as_cmap=True)\n",
    "    # plot the heatmap\n",
    "    sns.heatmap(\n",
    "        corr_mat,\n",
    "        mask=mask,\n",
    "        cmap=cmap,\n",
    "        vmax=0.3,\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.5},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Correlation Matrix\", fontsize=16)\n",
    "    # change back to darkgrid style\n",
    "    sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7238b-b6be-42b6-8325-54482c1adcd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T21:13:16.995297Z",
     "start_time": "2019-12-08T21:13:13.456922Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_mat = df.select_dtypes(include=\"number\").corr()\n",
    "plot_correlation_matrix(corr_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88774c18-69fd-4fb1-803d-9a7f8e55b02a",
   "metadata": {},
   "source": [
    "We can also directly inspect the correlation between the features (numerical) and the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308abfa-8698-46b6-b034-51e39dfe56c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:56:14.630145Z",
     "start_time": "2020-01-22T17:56:14.601671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"number\").corr()[['default payment next month']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3209a91-7bd1-496c-ac19-19f7869e911a",
   "metadata": {},
   "source": [
    "Plot the distribution of limit balance for each gender and education level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86f76b-30e9-4c45-bf19-def384dc271e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:56:30.022284Z",
     "start_time": "2020-01-22T17:56:25.037383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x=\"EDUCATION\", y=\"LIMIT_BAL\", hue=\"SEX\", split=True, data=df)\n",
    "ax.set_title(\"Distribution of limit balance per education level\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae26867-b82d-43e5-b02a-f02becd3ddd4",
   "metadata": {},
   "source": [
    "The following code plots the same information, without splitting the violin plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662e2e3-36c5-4391-9a35-ab284aaf86d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T11:58:22.541157Z",
     "start_time": "2020-01-28T11:58:22.538964Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x='EDUCATION', y='LIMIT_BAL',\n",
    "                    hue='SEX', data=df)\n",
    "ax.set_title('Distribution of limit balance per education level',\n",
    "             fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df345d-9189-4f79-b566-d2375dff3003",
   "metadata": {},
   "source": [
    "Investigate the distribution of the target variable per gender and education level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30007d84-5632-4fee-a877-d92096564728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T17:56:49.986435Z",
     "start_time": "2020-01-22T17:56:45.478524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"default payment next month\", hue=\"SEX\", data=df, orient=\"h\")\n",
    "ax.set_title(\"Distribution of the target variable\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88a4e9-ac30-4b23-94e3-69dd7849157e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3e1ee-ee4b-4cfe-bcbf-339d364135e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"default payment next month\", hue=\"EDUCATION\", data=df, orient=\"h\")\n",
    "ax.set_title(\"Distribution of the target variable\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0edf64-8053-437c-afbc-382c61ff48de",
   "metadata": {},
   "source": [
    "## Splitting the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f09db-73a8-46b8-8e8e-5ccf2f76d840",
   "metadata": {},
   "source": [
    "Separe the features from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ed025-6e67-441f-9e0d-3f49e1efbc2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:03:11.188739Z",
     "start_time": "2020-01-28T13:03:10.701780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"default payment next month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a0a9b-eb10-4754-af18-a9cc1ea296a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ae3f5-c2ad-4cdf-87d9-4d9a48ff2531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.value_counts()/y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450d34a-bfd2-41eb-87ed-2153024702cf",
   "metadata": {},
   "source": [
    "Split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdc879-91dc-491d-a8f3-e451d2e47a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T12:28:13.909098Z",
     "start_time": "2020-01-28T12:28:13.898473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5475, stratify=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1faef3-bd4a-4a82-821b-a5b3508bc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cff60-76c8-4910-88e5-9cec09bbabf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5808e-9f69-4ccb-91a7-4c889cc6ece9",
   "metadata": {},
   "source": [
    "Is it ok to split this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f402f-924f-4ed7-92c3-3b46130c37f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621636c-6d7c-4480-be66-19a538fcfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sort_values(\"SEX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644f52a-d8cb-4d7f-a5b8-b98e6be2bec5",
   "metadata": {},
   "source": [
    "Split without shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be228e42-bc97-479e-ae79-13c79b8b6783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T12:28:14.235010Z",
     "start_time": "2020-01-28T12:28:14.225616Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87476d4-cb06-4558-ac60-440a8720b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad6c7c-956f-4f97-aee7-4eab235db77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c01df-c4b6-4783-a86d-f909239d8a3b",
   "metadata": {},
   "source": [
    "Verify that the ratio of the target is preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d622a0-6dfa-4c13-a041-c2e38d345022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T12:28:14.823687Z",
     "start_time": "2020-01-28T12:28:14.818357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dd8c8-cfa3-4215-917a-6b8177b72e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T12:28:15.302239Z",
     "start_time": "2020-01-28T12:28:15.296472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563c8aa-8a41-4baa-9998-3ead72b02c0d",
   "metadata": {},
   "source": [
    "### If we want a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e925e-5383-4f0c-9e75-c818d77b2fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T12:28:16.424492Z",
     "start_time": "2020-01-28T12:28:16.396682Z"
    }
   },
   "outputs": [],
   "source": [
    "# # define the size of the validation and test sets\n",
    "# VALID_SIZE = 0.1\n",
    "# TEST_SIZE = 0.2\n",
    "\n",
    "# # create the initial split - training and temp\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y,\n",
    "#                                                     test_size=(VALID_SIZE + TEST_SIZE),\n",
    "#                                                     random_state=42)\n",
    "\n",
    "# # calculate the new test size\n",
    "# NEW_TEST_SIZE = np.around(TEST_SIZE / (VALID_SIZE + TEST_SIZE), 2)\n",
    "\n",
    "# # create the valid and test sets\n",
    "# X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp,\n",
    "#                                                     test_size=NEW_TEST_SIZE,\n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99a42e-50e7-4815-b89a-86c618423fd4",
   "metadata": {},
   "source": [
    "## Fitting a single decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d89d0-81af-403a-bc28-592867ae4596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def performance_evaluation_report(\n",
    "    model, X_test, y_test, show_plot=False, labels=None, show_pr_curve=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Function for creating a performance report of a classification model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator\n",
    "        A fitted estimator for classification problems.\n",
    "    X_test : pd.DataFrame\n",
    "        DataFrame with features matching y_test\n",
    "    y_test : array/pd.Series\n",
    "        Target of a classification problem.\n",
    "    show_plot : bool\n",
    "        Flag whether to show the plot\n",
    "    labels : list\n",
    "        List with the class names.\n",
    "    show_pr_curve : bool\n",
    "        Flag whether to also show the PR-curve. For this to take effect,\n",
    "        show_plot must be True.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    stats : pd.Series\n",
    "        A series with the most important evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    if show_plot:\n",
    "        if labels is None:\n",
    "            labels = [\"Negative\", \"Positive\"]\n",
    "\n",
    "        N_SUBPLOTS = 3 if show_pr_curve else 2\n",
    "        PLOT_WIDTH = 15 if show_pr_curve else 12\n",
    "        PLOT_HEIGHT = 5 if show_pr_curve else 6\n",
    "\n",
    "        fig, ax = plt.subplots(1, N_SUBPLOTS, figsize=(PLOT_WIDTH, PLOT_HEIGHT))\n",
    "        fig.suptitle(\"Performance Evaluation\", fontsize=16)\n",
    "\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            linewidths=0.5,\n",
    "            cmap=\"BuGn_r\",\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            ax=ax[0],\n",
    "            annot_kws={\"ha\": \"center\", \"va\": \"center\"},\n",
    "        )\n",
    "        ax[0].set(\n",
    "            xlabel=\"Predicted label\", ylabel=\"Actual label\", title=\"Confusion Matrix\"\n",
    "        )\n",
    "        ax[0].xaxis.set_ticklabels(labels)\n",
    "        ax[0].yaxis.set_ticklabels(labels)\n",
    "\n",
    "        ax[1].plot(fpr, tpr, \"b-\", label=f\"ROC-AUC = {roc_auc:.2f}\")\n",
    "        ax[1].set(\n",
    "            xlabel=\"False Positive Rate\", ylabel=\"True Positive Rate\", title=\"ROC Curve\"\n",
    "        )\n",
    "        ax[1].plot(\n",
    "            fp / (fp + tn), tp / (tp + fn), \"ro\", markersize=8, label=\"Decision Point\"\n",
    "        )\n",
    "        ax[1].plot([0, 1], [0, 1], \"r--\")\n",
    "        ax[1].legend(loc=\"lower right\")\n",
    "\n",
    "        if show_pr_curve:\n",
    "            ax[2].plot(recall, precision, label=f\"PR-AUC = {pr_auc:.2f}\")\n",
    "            ax[2].set(\n",
    "                xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall Curve\"\n",
    "            )\n",
    "            ax[2].legend()\n",
    "\n",
    "    stats = {\n",
    "        \"accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "        \"precision\": metrics.precision_score(y_test, y_pred),\n",
    "        \"recall\": metrics.recall_score(y_test, y_pred),\n",
    "        \"specificity\": (tn / (tn + fp)),\n",
    "        \"f1_score\": metrics.f1_score(y_test, y_pred),\n",
    "        \"cohens_kappa\": metrics.cohen_kappa_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5e76c-6724-4f33-8168-0deb1dfee7c5",
   "metadata": {},
   "source": [
    "Create the instance of the model, fit it to the training data and create prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc0e0a-4680-4ef3-bdfd-6c28d25a6a02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:08.861412Z",
     "start_time": "2020-01-28T13:40:08.333620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "y_pred = tree_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5907d72-20dc-47ed-89ff-7495e1e92572",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea92e1b-a217-4f2e-985e-8d6fe821f521",
   "metadata": {},
   "source": [
    "Evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6dc7e-008d-4641-b3a9-eec914e250ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.011916Z",
     "start_time": "2020-01-28T13:40:10.691002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS = [\"No Default\", \"Default\"]\n",
    "tree_perf = performance_evaluation_report(\n",
    "    tree_classifier, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021b83a-d330-4a15-adca-60b5cf3ca05e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.834306Z",
     "start_time": "2020-01-28T13:40:19.829937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656842a8-151c-4d4a-9ecc-9baee7096c63",
   "metadata": {},
   "source": [
    "Plot the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb16cf-015c-4c5e-921e-a8399bb64a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T18:02:40.136958Z",
     "start_time": "2020-01-22T18:02:39.213199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "small_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "small_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(small_tree, feature_names=list(X_train.columns), class_names=LABELS, rounded=True, proportion=False, precision=2, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63139683-92f5-4639-b94b-a0fa1b32c51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:47:42.346028Z",
     "start_time": "2020-01-28T13:47:42.337451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_prob = tree_classifier.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80feee17-9a3f-4dc4-971a-a6d44e6807d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:47:43.430290Z",
     "start_time": "2020-01-28T13:47:43.425430Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a259fb-ee1b-4533-a2dd-8f8140ca8196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:47:53.269588Z",
     "start_time": "2020-01-28T13:47:49.719448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.plot(recall, precision, label=f\"PR-AUC = {metrics.auc(recall, precision):.2f}\")\n",
    "ax.set(title=\"Precision-Recall Curve\", xlabel=\"Recall\", ylabel=\"Precision\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3cdc5c-6c81-4572-9ce4-2a1685d3b54b",
   "metadata": {},
   "source": [
    "## Fitting a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d573f272-c39a-4e6b-949a-2737da3fe515",
   "metadata": {},
   "source": [
    "Create the instance of the model, fit it to the training data and create prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5237dcb-3cf7-411b-a863-4f37778cf691",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d785a0-5df1-4b6d-bae2-68f8e4a67e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:08.861412Z",
     "start_time": "2020-01-28T13:40:08.333620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, max_features=10, n_jobs=-1, random_state=42\n",
    ")\n",
    "%time rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d7bf6-d01f-4498-895c-cf61c1e7d001",
   "metadata": {},
   "source": [
    "Evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801aa3ab-dd09-4486-898e-08817779f55a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.011916Z",
     "start_time": "2020-01-28T13:40:10.691002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_perf = performance_evaluation_report(\n",
    "    rf_classifier, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93edd5e-feb2-4735-861b-423811d30788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.834306Z",
     "start_time": "2020-01-28T13:40:19.829937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef2213-70c8-4fd4-a1c3-5133eb13a7e2",
   "metadata": {},
   "source": [
    "## Fitting Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ed2a3-6b47-48c2-9069-e4ecf3060ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    n_estimators=50, learning_rate=0.1, random_state=0\n",
    ")\n",
    "%time adaboost_classifier.fit(X_train, y_train)\n",
    "y_pred_adaboost = adaboost_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bcb07-48d6-43f8-92a2-1061ba44b732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adaboost_perf = performance_evaluation_report(\n",
    "    adaboost_classifier, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2f354-b9a6-4dd2-8878-8ffe760165be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adaboost_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45508f31-8b4b-48eb-af52-4c81f1ab7874",
   "metadata": {},
   "source": [
    "## Fitting a Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f6910-764f-40d2-9a7b-252bb60084f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:08.861412Z",
     "start_time": "2020-01-28T13:40:08.333620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "boost_classifier = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, max_depth=3, n_estimators=50, random_state=0\n",
    ")\n",
    "%time boost_classifier.fit(X_train, y_train)\n",
    "y_pred_boost = boost_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4a890-014d-40e3-b96f-90a055be156e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.011916Z",
     "start_time": "2020-01-28T13:40:10.691002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "boost_perf = performance_evaluation_report(\n",
    "    boost_classifier, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3599544-e399-46a9-9d8a-c390d095e3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T13:40:19.834306Z",
     "start_time": "2020-01-28T13:40:19.829937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "boost_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970dd2d-897e-4448-a56f-fa311dc4040a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier(\n",
    "    learning_rate=0.1, max_depth=3, n_estimators=50, random_state=0\n",
    ")\n",
    "%time xgb_classifier.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b231829-3eb9-4246-adf3-dbd09f984dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_perf = performance_evaluation_report(\n",
    "    xgb_classifier, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848663d-742e-4b75-beb0-9483eabb9bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888865a1-6ab8-4e79-bd81-c6ab9080fca0",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters \n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "The code below is designed to visualize how various cross-validation (CV) strategies from `scikit-learn` work. In this specific example, the same X, y, and groups are used to demonstrate the different behaviors of CV strategies. The idea is to show how each CV strategy divides the same dataset into training and test sets, given the same target labels (y) and group labels (groups).\n",
    "\n",
    "The target labels (y) and group labels (groups) are visualized at the bottom of each plot to show what they look like relative to the CV splits. This makes it easier to understand how each CV strategy works in the context of the class labels and groupings. It also helps to show how strategies like StratifiedKFold or GroupKFold use this additional information (y and groups respectively) to create splits.\n",
    "\n",
    "For example:\n",
    "\n",
    "`StratifiedKFold` ensures that the proportion of each class in the target variable is the same in both the training and test sets.\n",
    "\n",
    "`GroupKFold` ensures that the same group is not present in both training and test sets.\n",
    "\n",
    "By using the same y and groups for each CV strategy, it becomes easier to compare and contrast how they each work and how they handle the classes and groups. In real-world applications, you would choose the most appropriate CV strategy based on the specific characteristics of your data (e.g., if you have imbalanced classes, grouped data, time series data, etc.).\n",
    "\n",
    "**What are those groups?**\n",
    "\n",
    "\n",
    "In some machine learning applications, the data might have a group structure. For example, if you have medical data, multiple samples might come from the same patient. Or in finance, multiple data points might come from the same company or the same time period. This introduces correlation among the samples.\n",
    "\n",
    "In such scenarios, it's important to ensure that all samples corresponding to a single group are either in the training set or in the test set but not both, in order to get an unbiased estimate of the generalization performance. This is known as group-wise or group-based cross-validation.\n",
    "\n",
    "GroupKFold, GroupShuffleSplit, and other such cross-validation techniques from scikit-learn expect an additional groups array that specifies the group labels for each sample. The groups parameter ensures that the same group is not represented in both the training and test sets. This helps to prevent data leakage and results in a more robust model evaluation.\n",
    "\n",
    "For instance, if you have data from 10 patients and each patient has 10 samples, the groups array would look something like [1, 1, 1, ..., 2, 2, ..., 10, 10], where the number indicates the patient ID for each sample.\n",
    "\n",
    "To summarize, the groups parameter is used to specify which samples belong to the same \"group,\" so that during the splitting process, samples from the same group are kept together, either entirely in the training set or in the test/validation set.\n",
    "\n",
    "**What is the stratification?**\n",
    "\n",
    "Certainly. In machine learning, it's often important to ensure that the training and test sets have similar properties. One such property might be the distribution of the target labels (y).\n",
    "\n",
    "In a stratified sampling approach, the training and test sets are constructed so that they have approximately the same distribution of target labels as the complete dataset. This is particularly useful when the target labels are imbalanced; for example, in binary classification problems where one class is much less frequent than the other.\n",
    "\n",
    "In StratifiedKFold, each fold is made by preserving the percentage of samples for each class. The algorithm ensures that each fold has the same distribution of the target labels as the entire dataset. To accomplish this, it needs access to the target labels (y) to know how to perform the stratified sampling.\n",
    "\n",
    "So, when using StratifiedKFold or any stratified sampling technique, you must provide y (the target labels) so that the cross-validator can arrange the folds to have a similar distribution of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95903cb7-6436-4d79-bba1-d0e94e7bb9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (TimeSeriesSplit, RepeatedKFold, KFold, ShuffleSplit,\n",
    "                                     StratifiedKFold, GroupShuffleSplit,\n",
    "                                     GroupKFold, StratifiedShuffleSplit)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "np.random.seed(3)\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "n_splits = 5\n",
    "\n",
    "# Generate the class/group data\n",
    "n_points = 100\n",
    "X = np.random.randn(100, 10)\n",
    "\n",
    "percentiles_classes = [.1, .3, .6]\n",
    "y = np.hstack([[ii] * int(100 * perc)\n",
    "               for ii, perc in enumerate(percentiles_classes)])\n",
    "\n",
    "# Evenly spaced groups repeated once\n",
    "groups = np.hstack([[ii] * 10 for ii in range(10)])\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "        \n",
    "    yticklabels = list(range(n_splits))\n",
    "    \n",
    "    add_yticks = 0\n",
    "    if y is not None:\n",
    "        # Plot the data classes and groups at the end\n",
    "        ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "                   c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "        yticklabels = yticklabels + ['class']\n",
    "        add_yticks = add_yticks + 1\n",
    "    \n",
    "    if group is not None:\n",
    "        ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "                   c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "        yticklabels = yticklabels + ['group']\n",
    "        add_yticks = add_yticks + 1\n",
    "\n",
    "    # Formatting\n",
    "\n",
    "    ax.set(yticks=np.arange(n_splits+add_yticks) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+add_yticks+0.2, -.2], xlim=[0, 100])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "    return ax\n",
    "\n",
    "cvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,\n",
    "       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]\n",
    "\n",
    "\n",
    "for cv in cvs:\n",
    "    this_cv = cv(n_splits=n_splits)\n",
    "    fig, ax = plt.subplots(figsize=(18, 6))\n",
    "    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n",
    "\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "    # Make the legend fit\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99344b-f5f4-4829-b490-8f3266b2d2ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:24:41.175443Z",
     "start_time": "2020-01-28T16:24:41.172637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfe952-abc7-442e-b65e-1743316aba5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894ef83-304c-4a86-a9ef-06299c0a8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, max_features=10, n_jobs=-1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa149b-dd44-4500-9d5f-61431fe5471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9838-db87-4a49-88ec-6c895186cc67",
   "metadata": {},
   "source": [
    "Evaluate the random forest regressor using cross-validation (but you can do the same for whatever classifier we have used so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c15cff-eaf3-4d67-8724-1d198c876819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:24:46.987870Z",
     "start_time": "2020-01-28T16:24:41.611076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_val_score(rf_classifier, X_train, y_train, cv=k_fold, scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0cb5d1-38fa-4594-a1aa-6c916ad403e2",
   "metadata": {},
   "source": [
    "Add extra metrics to cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f287d-8748-4b8f-998c-3c43b125cc43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:25:05.879676Z",
     "start_time": "2020-01-28T16:24:59.127144Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_validate(\n",
    "    rf_classifier,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=k_fold,\n",
    "    scoring=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1f9c6-8888-4e0a-ae44-d7c162c40b25",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84df23a-c33e-4305-818e-ad3e70ddaeff",
   "metadata": {},
   "source": [
    "Define the parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa1fddb-083a-45b0-b978-4193c50d3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(\n",
    "     max_features=10, n_jobs=-1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ea9d1-02fa-46f3-a2bd-4c204b1eea57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:25:06.004404Z",
     "start_time": "2020-01-28T16:25:06.001060Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"max_depth\": range(3, 6),\n",
    "    # 'min_samples_leaf': range(2, 6),\n",
    "    'n_estimators' : [10,20],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203de82-ac3c-472e-8ddf-eaaa57dd0463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee164f72-417f-40ab-9d8d-d8d7bee8afd1",
   "metadata": {},
   "source": [
    "Run Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f0e4a-bae7-46c5-b0b0-aa052c04d877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:31:31.961367Z",
     "start_time": "2020-01-28T16:25:06.128211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_gs = GridSearchCV(\n",
    "    rf_classifier, param_grid, scoring=\"recall\", cv=k_fold, n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "classifier_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7830f9-b4b8-4ad2-8308-9007132f90e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:31:32.154973Z",
     "start_time": "2020-01-28T16:31:32.116211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Best parameters: {classifier_gs.best_params_}\")\n",
    "print(f\"Recall (Training set): {classifier_gs.best_score_:.4f}\")\n",
    "print(\n",
    "    f\"Recall (Test set): {metrics.recall_score(y_test, classifier_gs.predict(X_test)):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf43e4-3774-42d2-9ee1-76962ab158bb",
   "metadata": {},
   "source": [
    "Evaluate the performance of the Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239d725-72f6-40fa-bb25-5ec82ed3b3b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:31:42.049117Z",
     "start_time": "2020-01-28T16:31:32.317422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LABELS = [\"No Default\", \"Default\"]\n",
    "tree_gs_perf = performance_evaluation_report(\n",
    "    classifier_gs, X_test, y_test, labels=LABELS, show_plot=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/ch8_im20.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de0754-06e2-40cf-9457-b03e39010bfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:31:42.240641Z",
     "start_time": "2020-01-28T16:31:42.236741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_gs_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f0ea8-fe7d-4d5a-a6b3-72783a87f586",
   "metadata": {},
   "source": [
    "Run Randomized Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccebca-52e9-464d-a1e6-531764df8459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:33:55.367212Z",
     "start_time": "2020-01-28T16:31:42.420281Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_rs = RandomizedSearchCV(rf_classifier, param_grid, scoring='recall',\n",
    "                                   cv=k_fold, n_jobs=-1, verbose=1,\n",
    "                                   n_iter=100, random_state=42)\n",
    "classifier_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007109d1-f7cc-4195-8aa1-119b864d62aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:33:55.661198Z",
     "start_time": "2020-01-28T16:33:55.613345Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Best parameters: {classifier_rs.best_params_}')\n",
    "print(f'Recall (Training set): {classifier_rs.best_score_:.4f}')\n",
    "print(f'Recall (Test set): {metrics.recall_score(y_test, classifier_rs.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de95444-5f53-4ae2-9606-d2f47a5ecfa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:34:05.588141Z",
     "start_time": "2020-01-28T16:33:55.905898Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_rs_perf = performance_evaluation_report(classifier_rs, X_test,\n",
    "                                             y_test, labels=LABELS,\n",
    "                                             show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ca732-522e-4777-bb66-94f756ccc19d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-28T16:34:05.808345Z",
     "start_time": "2020-01-28T16:34:05.804213Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_rs_perf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
